{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import logging\n",
    "import json\n",
    "import pickle\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    average_precision_score,\n",
    ")\n",
    "from scipy.stats import gmean\n",
    "\n",
    "# import gensim libraries\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "from gensim.parsing.preprocessing import preprocess_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directories(directories):\n",
    "    for directory in directories:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "            print(f\"Created directory: {directory}\")\n",
    "\n",
    "\n",
    "def extract_pmid(links):\n",
    "    return [link.split(\"/\")[-1] for link in links]\n",
    "\n",
    "\n",
    "def prepare_dataset(dataset_name, dataset_dir, doc_df):\n",
    "    dfs = []\n",
    "    for json_file in glob.glob(os.path.join(dataset_dir, \"*.json\")):\n",
    "        with open(json_file) as fp:\n",
    "            json_data = \"\".join(fp)\n",
    "        data = json.loads(json_data)\n",
    "        data = data[\"questions\"]\n",
    "        dfs.append(pd.DataFrame(data))\n",
    "    df = pd.concat(dfs)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    df[\"pmids\"] = df[\"documents\"].apply(extract_pmid)\n",
    "    pmids = list(chain.from_iterable(df[\"pmids\"].to_list()))\n",
    "    print(\n",
    "        f\"total number of unique docs provided in {dataset_name}: {len(set(pmids))}\"\n",
    "    )\n",
    "    logger.info(\n",
    "        \"total number of unique docs provided in %s: %d\",\n",
    "        dataset_name,\n",
    "        len(set(pmids)),\n",
    "    )\n",
    "\n",
    "    corpus_df = doc_df[doc_df[\"pmid\"].isin(pmids)]\n",
    "    print(f\"num of docs found in corpus:{corpus_df.shape[0]}\")\n",
    "    logger.info(\"num of docs found in corpus: %d\", corpus_df.shape[0])\n",
    "\n",
    "    def filter_pmid(pmids):\n",
    "        filtered_pmids = [\n",
    "            pmid for pmid in pmids if pmid in doc_df[\"pmid\"].to_list()\n",
    "        ]\n",
    "        return filtered_pmids\n",
    "\n",
    "    df[\"pmids_found\"] = df[\"pmids\"].apply(filter_pmid)\n",
    "\n",
    "    filtered_df = df[df[\"pmids_found\"].apply(len) > 0]\n",
    "    # average number of docs per query\n",
    "    total_num_docs = sum(filtered_df[\"pmids_found\"].apply(len))\n",
    "    total_num_queries = filtered_df[\"body\"].shape[0]\n",
    "    avg_num_docs_per_query = total_num_docs / total_num_queries\n",
    "    print(f\"avg num of docs per query: {avg_num_docs_per_query}\")\n",
    "    logger.info(\"avg num of docs per query: %d\", avg_num_docs_per_query)\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "# retrieve the top N similar documents for a given document or query\n",
    "def retrieve_documents(query, lda_model, sim_matrix, dictionary, topn=10):\n",
    "    vec_bow = dictionary.doc2bow(query)\n",
    "    vec_lda = lda_model[vec_bow]\n",
    "    sims = sim_matrix[vec_lda]\n",
    "    sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "    return sims[:topn]\n",
    "\n",
    "\n",
    "# get pmids from doc indexes\n",
    "def get_pmids_from_doc_indexes(doc_indexes, doc_df):\n",
    "    return [doc_df[\"pmid\"].iloc[doc_idx[0]] for doc_idx in doc_indexes]\n",
    "\n",
    "\n",
    "# def calculate_metrics(df, true_col, pred_col):\n",
    "#     # Calculate precision, recall, f1, and average precision for each row\n",
    "#     df[\"precision\"] = 0\n",
    "#     df[\"recall\"] = 0\n",
    "#     df[\"f1\"] = 0\n",
    "#     df[\"avg_precision\"] = 0\n",
    "\n",
    "#     for i in range(len(df)):\n",
    "#         # Fit MultiLabelBinarizer on each row separately\n",
    "#         mlb = MultiLabelBinarizer()\n",
    "#         mlb.fit(\n",
    "#             [df[true_col].iloc[i] + df[pred_col].iloc[i]]\n",
    "#         )  # Combining true and predicted labels\n",
    "\n",
    "#         # Transform true and predicted columns separately\n",
    "#         X_true = mlb.transform([df[true_col].iloc[i]])\n",
    "#         X_pred = mlb.transform([df[pred_col].iloc[i]])\n",
    "\n",
    "#         # Calculate precision, recall, f1, and average precision for the current row\n",
    "#         df.at[i, \"precision\"] = precision_score(\n",
    "#             X_true[0], X_pred[0], zero_division=0\n",
    "#         )\n",
    "#         df.at[i, \"recall\"] = recall_score(X_true[0], X_pred[0], zero_division=0)\n",
    "#         df.at[i, \"f1\"] = f1_score(X_true[0], X_pred[0], zero_division=0)\n",
    "#         df.at[i, \"avg_precision\"] = average_precision_score(\n",
    "#             X_true[0], X_pred[0]\n",
    "#         )\n",
    "\n",
    "#     # Calculate mean precision, mean recall, and mean f1\n",
    "#     mean_precision = df[\"precision\"].mean()\n",
    "#     mean_recall = df[\"recall\"].mean()\n",
    "#     mean_f1 = df[\"f1\"].mean()\n",
    "\n",
    "#     # Calculate MAP and GMAP\n",
    "#     map_score = df[\"avg_precision\"].mean()\n",
    "#     gmap_score = gmean(df[\"avg_precision\"])\n",
    "\n",
    "#     # Create a new dataframe to store the mean scores\n",
    "#     mean_scores_df = pd.DataFrame(\n",
    "#         {\n",
    "#             \"mean_precision\": mean_precision,\n",
    "#             \"mean_recall\": mean_recall,\n",
    "#             \"mean_f1\": mean_f1,\n",
    "#             \"MAP\": map_score,\n",
    "#             \"GMAP\": gmap_score,\n",
    "#         },\n",
    "#         index=[0],\n",
    "#     )\n",
    "\n",
    "#     # Return both dataframes\n",
    "#     return mean_scores_df\n",
    "\n",
    "\n",
    "def average_precision(retrieved_items, relevant_items):\n",
    "    ap = 0.0\n",
    "    num_relevant = 0\n",
    "\n",
    "    for r in range(len(retrieved_items)):\n",
    "        if retrieved_items[r] in relevant_items:\n",
    "            num_relevant += 1\n",
    "            precision_at_r = num_relevant / (r + 1)\n",
    "            ap += precision_at_r\n",
    "    \n",
    "    ap /= min(len(retrieved_items), 10)\n",
    "    \n",
    "    # add 0.0001 to avoid division by zero\n",
    "    ap += 0.0001    \n",
    "    return ap\n",
    "\n",
    "def calculate_metrics(df, true_col, pred_col):\n",
    "    # Calculate precision, recall, f1, and average precision for each row\n",
    "    df[\"precision\"] = 0\n",
    "    df[\"recall\"] = 0\n",
    "    df[\"f1\"] = 0\n",
    "    df[\"avg_precision\"] = 0\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        true_items = set(df[true_col].iloc[i])\n",
    "        pred_items = set(df[pred_col].iloc[i])\n",
    "        \n",
    "        intersection = true_items.intersection(pred_items)\n",
    "        union = true_items.union(pred_items)\n",
    "        \n",
    "        # Calculate precision, recall, and f1 for the current row\n",
    "        precision = len(intersection) / len(pred_items) if len(pred_items) > 0 else 0\n",
    "        recall = len(intersection) / len(true_items) if len(true_items) > 0 else 0\n",
    "        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        df.at[i, \"precision\"] = precision\n",
    "        df.at[i, \"recall\"] = recall\n",
    "        df.at[i, \"f1\"] = f1\n",
    "\n",
    "        # Calculate average precision using the provided function\n",
    "        df.at[i, \"avg_precision\"] = average_precision(df[pred_col].iloc[i], df[true_col].iloc[i])\n",
    "\n",
    "    # Calculate mean precision, mean recall, and mean f1\n",
    "    mean_precision = df[\"precision\"].mean()\n",
    "    mean_recall = df[\"recall\"].mean()\n",
    "    mean_f1 = df[\"f1\"].mean()\n",
    "\n",
    "    # Calculate MAP and GMAP\n",
    "    map_score = df[\"avg_precision\"].mean()\n",
    "    gmap_score = gmean(df[\"avg_precision\"])\n",
    "\n",
    "    # Create a new dataframe to store the mean scores\n",
    "    mean_scores_df = pd.DataFrame(\n",
    "        {\n",
    "            \"mean_precision\": mean_precision,\n",
    "            \"mean_recall\": mean_recall,\n",
    "            \"mean_f1\": mean_f1,\n",
    "            \"MAP\": map_score,\n",
    "            \"GMAP\": gmap_score,\n",
    "        },\n",
    "        index=[0],\n",
    "    )\n",
    "\n",
    "    # Return the mean scores dataframe\n",
    "    return mean_scores_df\n",
    "\n",
    "def get_max(logs, metric):\n",
    "    df = pd.concat(logs)\n",
    "    print(f\"max {metric}:\")\n",
    "    df = df.sort_values(by=[metric], ascending=False)\n",
    "    # return df[df[metric] == df[metric].max()]\n",
    "    return df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    # load dir_dict from json file in home directory\n",
    "    home_dir = os.path.expanduser(\"~\")\n",
    "    with open(f\"{home_dir}/.biomedqa_dir.json\", encoding=\"utf-8\") as fp:\n",
    "        dir_dict = json.load(fp)\n",
    "except Exception as exc:\n",
    "    print(\"Error: unable to load directory dictionary. Please run setup.py\")\n",
    "    raise exc\n",
    "\n",
    "# set directories\n",
    "BASE_DIR = dir_dict[\"base_dir\"]\n",
    "DATA_DIR = dir_dict[\"data_dir\"]\n",
    "MODEL_DIR = dir_dict[\"model_dir\"]\n",
    "LOG_DIR = dir_dict[\"log_dir\"]\n",
    "RESULTS_DIR = dir_dict[\"results_dir\"]\n",
    "\n",
    "DATASET = \"bioasq\"\n",
    "YEAR = \"2022\"\n",
    "__file__ = \"notebooks/evaluate_by_batch.ipynb\"\n",
    "\n",
    "TRAIN_DATASET_NAME = \"Task10BGoldenEnriched\"\n",
    "TRAIN_DATASET_DIR = f\"{DATA_DIR}/raw/{DATASET}/{YEAR}/{TRAIN_DATASET_NAME}\"\n",
    "TRAIN_DOC_DIR = (\n",
    "    f\"{DATA_DIR}/processed/{DATASET}/{YEAR}/{TRAIN_DATASET_NAME}_documents/\"\n",
    ")\n",
    "print(f\"train dataset name:{TRAIN_DATASET_NAME}\")\n",
    "print(f\"train dataset dir:{TRAIN_DATASET_DIR}\")\n",
    "print(f\"train doc dir:{TRAIN_DOC_DIR}\")\n",
    "\n",
    "TEST_DATASET_NAME = \"Task10BGoldenEnriched\"\n",
    "TEST_DATASET_DIR = f\"{DATA_DIR}/raw/{DATASET}/{YEAR}/{TEST_DATASET_NAME}\"\n",
    "TEST_DOC_DIR = (\n",
    "    f\"{DATA_DIR}/processed/{DATASET}/{YEAR}/{TEST_DATASET_NAME}_documents/\"\n",
    ")\n",
    "print(f\"test dataset name:{TEST_DATASET_NAME}\")\n",
    "print(f\"test dataset dir:{TEST_DATASET_DIR}\")\n",
    "print(f\"test doc dir:{TEST_DOC_DIR}\")\n",
    "\n",
    "# get file directory\n",
    "FILE_DIR = os.path.dirname(os.path.relpath(__file__))\n",
    "\n",
    "# set log dir directory according to current file directory\n",
    "LOG_DIR = f\"{LOG_DIR}/{FILE_DIR}\"\n",
    "print(f\"log dir:{LOG_DIR}\")\n",
    "\n",
    "# set model directory according to current file directory\n",
    "MODEL_DIR = f\"{MODEL_DIR}/{FILE_DIR}/{DATASET}/{YEAR}/{TRAIN_DATASET_NAME}/\"\n",
    "print(f\"model dir:{MODEL_DIR}\")\n",
    "\n",
    "# set results directory according to current file directory\n",
    "RESULTS_DIR = f\"{RESULTS_DIR}/{FILE_DIR}/{DATASET}/{YEAR}/{TRAIN_DATASET_NAME}/\"\n",
    "print(f\"results dir:{RESULTS_DIR}\")\n",
    "\n",
    "# create directories\n",
    "create_directories([LOG_DIR, MODEL_DIR, RESULTS_DIR])\n",
    "\n",
    "# set log file name\n",
    "log_file = os.path.join(\n",
    "    LOG_DIR, os.path.basename(__file__).split(\".\")[0] + \".log\"\n",
    ")\n",
    "print(f\"LOG_FILE: {log_file}\")\n",
    "\n",
    "# initialize logger\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=log_file,\n",
    "    format=\"%(process)d\\t%(asctime)s\\t%(levelname)s\\t%(message)s\",\n",
    "    level=logging.DEBUG,\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"Logger initialized\")\n",
    "\n",
    "# load documents\n",
    "logger.info(\"loading documents\")\n",
    "train_doc_df = pd.read_pickle(\n",
    "    f\"{TRAIN_DOC_DIR}{TRAIN_DATASET_NAME}_documents_df.pkl\"\n",
    ")\n",
    "\n",
    "test_doc_df = pd.read_pickle(\n",
    "    f\"{TEST_DOC_DIR}{TEST_DATASET_NAME}_documents_df.pkl\"\n",
    ")\n",
    "\n",
    "train_filtered_df = prepare_dataset(\n",
    "    TRAIN_DATASET_NAME, TRAIN_DATASET_DIR, train_doc_df\n",
    ")\n",
    "test_filtered_df = prepare_dataset(\n",
    "    TEST_DATASET_NAME, TEST_DATASET_DIR, test_doc_df\n",
    ")\n",
    "\n",
    "# preprocess documents using gensim's preprocess_documents function\n",
    "logger.info(\"preprocessing train documents\")\n",
    "train_doc_df[\"abstractText_preprocessed\"] = preprocess_documents(\n",
    "    train_doc_df[\"abstractText\"]\n",
    ")\n",
    "logger.info(\"preprocessing test documents\")\n",
    "test_doc_df[\"abstractText_preprocessed\"] = preprocess_documents(\n",
    "    test_doc_df[\"abstractText\"]\n",
    ")\n",
    "\n",
    "# Create a dictionary from the preprocessed documents of the training set\n",
    "logger.info(\"creating dictionary\")\n",
    "dictionary = corpora.Dictionary(train_doc_df[\"abstractText_preprocessed\"])\n",
    "\n",
    "# create bag of words corpus of the training set\n",
    "logger.info(\"creating bag of words for train documents\")\n",
    "train_corpus = [\n",
    "    dictionary.doc2bow(text)\n",
    "    for text in train_doc_df[\"abstractText_preprocessed\"]\n",
    "]\n",
    "# Create bag of words corpus of the test set\n",
    "logger.info(\"creating bag of words for test documents\")\n",
    "test_corpus = [\n",
    "    dictionary.doc2bow(text)\n",
    "    for text in test_doc_df[\"abstractText_preprocessed\"]\n",
    "]\n",
    "\n",
    "# preprocess questions\n",
    "logger.info(\"preprocessing test questions\")\n",
    "test_filtered_df[\"body_preprocessed\"] = preprocess_documents(\n",
    "    test_filtered_df[\"body\"].to_list()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of parameters\n",
    "list_of_parameters = {\"Name\": \"optimize_lda_Task10BGoldenEnriched_on_Task10BGoldenEnriched_TPESampler_v1\" \"Params\": [num_topics: 1367, chunksize: 758, passes: 39, iterations: 3, update_every=0] \"score\": 0.29571453066823394\n",
    "\"Name\": \"optimize_lda_Task10BGoldenEnriched_on_Task10BGoldenEnriched_TPESampler_v2\" \"Params\":[num_topics: 1423, chunksize: 2568, passes: 37, iterations: 94, update_every=1] \"score\": 0.29411506303481627}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train LDA model\n",
    "lda_model = LdaModel(\n",
    "    corpus=train_corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=1367,\n",
    "    chunksize=758,\n",
    "    passes=39,\n",
    "    update_every=0,\n",
    "    alpha=\"symmetric\",\n",
    "    eta=\"symmetric\",\n",
    "    decay=0.5,\n",
    "    offset=1,\n",
    "    eval_every=10,\n",
    "    iterations=3,\n",
    "    gamma_threshold=0.001,\n",
    "    minimum_probability=0.01,\n",
    "    random_state=1,\n",
    "    minimum_phi_value=0.01,\n",
    ")\n",
    "\n",
    "# Create a similarity matrix using the trained LDA model\n",
    "logger.info(\"creating similarity matrix\")\n",
    "sim_matrix = MatrixSimilarity(\n",
    "    lda_model[test_corpus], num_features=len(dictionary)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top 10 similar documents for each question\n",
    "logger.info(\"retrieving top similar documents for each question\")\n",
    "test_df = test_filtered_df.copy()\n",
    "test_df[\"top10_docs\"] = test_df[\"body_preprocessed\"].apply(\n",
    "    retrieve_documents, args=(lda_model, sim_matrix)\n",
    ")\n",
    "\n",
    "test_df[\"top10_pmids\"] = test_df[\"top10_docs\"].apply(\n",
    "    get_pmids_from_doc_indexes, args=(test_doc_df,)\n",
    ")\n",
    "\n",
    "# calculate metrics\n",
    "logger.info(\"calculating metrics\")\n",
    "eval_df_summary = calculate_metrics(test_df, \"pmids_found\", \"top10_pmids\")\n",
    "eval_df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate metrics for each batch\n",
    "logger.info(\"calculating metrics for each batch\")\n",
    "score_dfs = []\n",
    "for batch in glob.glob(f\"{TEST_DATASET_DIR}/*.json\"):\n",
    "    batch_name = os.path.basename(batch).split(\".\")[0]\n",
    "    with open(batch) as f:\n",
    "        batch_json = json.load(f)\n",
    "    batch_df = pd.DataFrame(batch_json[\"questions\"])\n",
    "    batch_df[\"body_preprocessed\"] = preprocess_documents(\n",
    "        batch_df[\"body\"].to_list()\n",
    "    )\n",
    "    batch_df[\"pmids\"] = batch_df[\"documents\"].apply(extract_pmid)\n",
    "    batch_df[\"top10_docs\"] = batch_df[\"body_preprocessed\"].apply(\n",
    "        retrieve_documents, args=(lda_model, sim_matrix)\n",
    "    )\n",
    "    batch_df[\"top10_pmids\"] = batch_df[\"top10_docs\"].apply(\n",
    "        get_pmids_from_doc_indexes, args=(test_doc_df,)\n",
    "    )\n",
    "    batch_eval_df = calculate_metrics(batch_df, \"pmids\", \"top10_pmids\")\n",
    "    batch_eval_df[\"batch\"] = batch_name\n",
    "    score_dfs.append(batch_eval_df)\n",
    "\n",
    "score_df = pd.concat(score_dfs)\n",
    "score_df = score_df.sort_values(by=[\"batch\"])\n",
    "score_df.set_index(\"batch\", inplace=True)\n",
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tmean_precision\tmean_recall\tmean_f1\tMAP\tGMAP\n",
    "batch\t\t\t\t\t\n",
    "9B1_golden\t0.320000\t0.498043\t0.311764\t0.401120\t0.283027\n",
    "9B2_golden\t0.330000\t0.534507\t0.315267\t0.402540\t0.287504\n",
    "9B3_golden\t0.320619\t0.464045\t0.304411\t0.398986\t0.275409\n",
    "9B4_golden\t0.295000\t0.526934\t0.297664\t0.367108\t0.262974\n",
    "9B5_golden\t0.354000\t0.497633\t0.334709\t0.429532\t0.306660"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize_lda_Task10BGoldenEnriched_on_Task10BGoldenEnriched_CmaEsSampler_v1\n",
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "logger.info(\"saving results\")\n",
    "eval_df_summary.to_csv(\n",
    "    f\"{RESULTS_DIR}{TRAIN_DATASET_NAME}_results.csv\", index=False\n",
    ")\n",
    "score_df.to_csv(\n",
    "    f\"{RESULTS_DIR}{TRAIN_DATASET_NAME}_results_by_batch.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation(BioASQ Script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retieve(query):\n",
    "    preprocessed_query = preprocess_documents([query])\n",
    "    topN_sims = retrieve_documents(preprocessed_query[0], lda_model, sim_matrix, dictionary, topn=10)\n",
    "    topN_pmids = get_pmids_from_doc_indexes(topN_sims, test_doc_df)\n",
    "    docs = [\"http://www.ncbi.nlm.nih.gov/pubmed/\" + str(pmid) for pmid in topN_pmids]\n",
    "    return docs\n",
    "\n",
    "retieve(\"what is the role of the DNA damage response in the pathogenesis of cancer?\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate metrics for each batch\n",
    "logger.info(\"calculating metrics for each batch\")\n",
    "scores = []\n",
    "for batch in glob.glob(f\"{TEST_DATASET_DIR}/*.json\"):\n",
    "    batch_name = os.path.basename(batch).split(\".\")[0]\n",
    "    print(batch_name)\n",
    "    with open(batch) as f:\n",
    "        json_data = json.load(f)[\"questions\"]\n",
    "    \n",
    "    predictions = []\n",
    "    for data in json_data:\n",
    "        data[\"documents\"] = retieve(data[\"body\"])\n",
    "        data[\"snippets\"] = []\n",
    "        data[\"ideal_answer\"] = []\n",
    "        data[\"exact_answer\"] = []\n",
    "        predictions.append(data)\n",
    "\n",
    "    # dump predictions to json file\n",
    "    predictions_json_file = f\"{RESULTS_DIR}/{batch_name}_predictions.json\"\n",
    "    with open(f\"{predictions_json_file}\", \"w\") as fp:\n",
    "        json.dump({\"questions\": predictions}, fp, indent=4)  \n",
    "    # evaluation_output = ! java -Xmx10G -cp \"/workspaces/biomed_qa-zparvez2z/Evaluation-Measures/flat/BioASQEvaluation/dist/*\" evaluation.EvaluatorTask1b -phaseA -e 5 $batch $predictions_json_file -verbose\n",
    "    # use os.popen instead of ! to get output\n",
    "    evaluation_output = os.popen(f\"java -Xmx10G -cp '/workspaces/biomed_qa-zparvez2z/Evaluation-Measures/flat/BioASQEvaluation/dist/*' evaluation.EvaluatorTask1b -phaseA -e 5 {batch} {predictions_json_file} -verbose\").read()\n",
    "    evaluation_output = evaluation_output.strip().split(\"\\n\")\n",
    "    score_dict = {\"Batch\": batch_name}\n",
    "    score_dict[batch_name] = evaluation_output[6:11]\n",
    "    for line in evaluation_output[1:]:\n",
    "        metric, score = line.split(\":\")\n",
    "        score_dict[metric.strip()] = float(score.strip())\n",
    "    scores.append(score_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = pd.DataFrame(scores)\n",
    "score_df = score_df.sort_values(by=[\"Batch\"])\n",
    "score_df[['Batch', 'MPrec documents', 'MRec documents', 'MF1 documents', 'MAP documents', 'GMAP documents']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df['MAP documents'].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
