{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "import pickle\n",
    "import glob\n",
    "import re\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    average_precision_score,\n",
    ")\n",
    "from scipy.stats import gmean\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "# import gensim libraries\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "from gensim.parsing.preprocessing import preprocess_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- include load_dataset and load_document inside prepare_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directories(directories):\n",
    "    for directory in directories:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "            print(f\"Created directory: {directory}\")\n",
    "\n",
    "\n",
    "def extract_pmid(links):\n",
    "    return [link.split(\"/\")[-1] for link in links]\n",
    "\n",
    "def dataset_info(dataset_dir):\n",
    "    datasets_info = []\n",
    "    # load json and create dataframe\n",
    "    for json_file in glob.glob(os.path.join(dataset_dir, \"*.json\")):\n",
    "        dataset_name = json_file.split(\"/\")[-1].split(\".\")[0]\n",
    "        with open(json_file) as fp:\n",
    "            json_data = \"\".join(fp)\n",
    "        data = json.loads(json_data)\n",
    "        data = data[\"questions\"]\n",
    "        df = pd.DataFrame(data)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        # rename 'body' column to 'query'\n",
    "        df.rename(columns={\"body\": \"query\"}, inplace=True)\n",
    "\n",
    "        # extract pmids\n",
    "        df[\"pmids\"] = df[\"documents\"].apply(extract_pmid)\n",
    "        pmids = list(chain.from_iterable(df[\"pmids\"].to_list()))\n",
    "\n",
    "        # query by type\n",
    "        queries_by_type = df.groupby(\"type\").size().to_dict()\n",
    "        info = {\n",
    "            \"dataset_name\": dataset_name,\n",
    "            \"num_queries\": df[\"query\"].shape[0],\n",
    "            \"num_docs\": len(set(pmids)),\n",
    "        }\n",
    "        info.update(queries_by_type)\n",
    "        datasets_info.append(info)\n",
    "    \n",
    "    # create dataframe from list of dictionaries\n",
    "    datasets_info_df = pd.DataFrame(datasets_info)\n",
    "    datasets_info_df.sort_values(by=\"dataset_name\", inplace=True)\n",
    "    return datasets_info_df\n",
    "\n",
    "def load_dataset(dataset_name, dataset_dir, logger):\n",
    "    # load json and create dataframe\n",
    "    dfs = []\n",
    "    for json_file in glob.glob(os.path.join(dataset_dir, \"*.json\")):\n",
    "        with open(json_file) as fp:\n",
    "            json_data = \"\".join(fp)\n",
    "        data = json.loads(json_data)\n",
    "        data = data[\"questions\"]\n",
    "        dfs.append(pd.DataFrame(data))\n",
    "    df = pd.concat(dfs)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # rename 'body' column to 'query'\n",
    "    df.rename(columns={\"body\": \"query\"}, inplace=True)\n",
    "    print(f\"{dataset_name} - query: {df['query'].shape[0]}\")\n",
    "    logger.info(\n",
    "        \"%s - query: %d\", dataset_name, df[\"query\"].shape[0]\n",
    "    )\n",
    "\n",
    "    # extract pmids and add to dataframe\n",
    "    df[\"pmids\"] = df[\"documents\"].apply(extract_pmid)\n",
    "    pmids = list(chain.from_iterable(df[\"pmids\"].to_list()))\n",
    "    print(\n",
    "        f\"{dataset_name} - unique docs: {len(set(pmids))}\"\n",
    "    )\n",
    "    logger.info(\n",
    "        \"%s - unique docs: %d\",\n",
    "        dataset_name,\n",
    "        len(set(pmids)),\n",
    "    )\n",
    "\n",
    "    # show number of query by type\n",
    "    print(f\"{dataset_name} - queries by type: {df.groupby('type').size()}\")\n",
    "    logger.info(\n",
    "        \"%s - queries by type: %s\",\n",
    "        dataset_name,\n",
    "        df.groupby(\"type\").size(),\n",
    "    )\n",
    "    return df, pmids\n",
    "\n",
    "\n",
    "def prepare_dataset(dataset_name, dataset_dir, doc_df, logger):\n",
    "    # load dataset\n",
    "    df, pmids= load_dataset(dataset_name, dataset_dir, logger)\n",
    "    \n",
    "    # filter out queries with no docs in corpus  \n",
    "    corpus_df = doc_df[doc_df[\"pmid\"].isin(pmids)]\n",
    "    print(f\"num of docs found in corpus:{corpus_df.shape[0]}\")\n",
    "    logger.info(\"num of docs found in corpus: %d\", corpus_df.shape[0])\n",
    "\n",
    "    def filter_pmid(pmids):\n",
    "        filtered_pmids = [\n",
    "            pmid for pmid in pmids if pmid in doc_df[\"pmid\"].to_list()\n",
    "        ]\n",
    "        return filtered_pmids\n",
    "\n",
    "    df[\"pmids_found\"] = df[\"pmids\"].apply(filter_pmid)\n",
    "\n",
    "    filtered_df = df[df[\"pmids_found\"].apply(len) > 0]\n",
    "    # average number of docs per query\n",
    "    total_num_docs = sum(filtered_df[\"pmids_found\"].apply(len))\n",
    "    total_num_queries = filtered_df[\"query\"].shape[0]\n",
    "    avg_num_docs_per_query = total_num_docs / total_num_queries\n",
    "    print(f\"{dataset_name} - docs per query: {avg_num_docs_per_query}\")\n",
    "    logger.info(\n",
    "        \"%s - docs per query: %d\",\n",
    "        dataset_name,\n",
    "        avg_num_docs_per_query,\n",
    "    )\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "# retrieve the top N similar documents for a given document or query\n",
    "def retrieve_doc_indexes(query, lda_model, sim_matrix, dictionary, topn=10):\n",
    "    vec_bow = dictionary.doc2bow(query)\n",
    "    vec_lda = lda_model[vec_bow]\n",
    "    sims = sim_matrix[vec_lda]\n",
    "    sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "    return sims[:topn]\n",
    "\n",
    "\n",
    "# get pmids from doc indexes\n",
    "def get_pmids_from_doc_indexes(doc_indexes, doc_df):\n",
    "    return [doc_df[\"pmid\"].iloc[doc_idx[0]] for doc_idx in doc_indexes]\n",
    "\n",
    "# get abstractText from doc indexes\n",
    "def get_text_from_doc_indexes(doc_indexes, doc_df):\n",
    "    return [doc_df[\"abstractText\"].iloc[doc_idx[0]] for doc_idx in doc_indexes]\n",
    "\n",
    "def calculate_metrics(df, true_col, pred_col):\n",
    "    # Calculate precision, recall, f1, and average precision for each row\n",
    "    df[\"precision\"] = 0\n",
    "    df[\"recall\"] = 0\n",
    "    df[\"f1\"] = 0\n",
    "    df[\"avg_precision\"] = 0\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        # Fit MultiLabelBinarizer on each row separately\n",
    "        mlb = MultiLabelBinarizer()\n",
    "        mlb.fit(\n",
    "            [df[true_col].iloc[i] + df[pred_col].iloc[i]]\n",
    "        )  # Combining true and predicted labels\n",
    "\n",
    "        # Transform true and predicted columns separately\n",
    "        X_true = mlb.transform([df[true_col].iloc[i]])\n",
    "        X_pred = mlb.transform([df[pred_col].iloc[i]])\n",
    "\n",
    "        # Calculate precision, recall, f1, and average precision for the current row\n",
    "        df.at[i, \"precision\"] = precision_score(\n",
    "            X_true[0], X_pred[0], zero_division=0\n",
    "        )\n",
    "        df.at[i, \"recall\"] = recall_score(X_true[0], X_pred[0], zero_division=0)\n",
    "        df.at[i, \"f1\"] = f1_score(X_true[0], X_pred[0], zero_division=0)\n",
    "        df.at[i, \"avg_precision\"] = average_precision_score(\n",
    "            X_true[0], X_pred[0]\n",
    "        )\n",
    "\n",
    "    # Calculate mean precision, mean recall, and mean f1\n",
    "    mean_precision = df[\"precision\"].mean()\n",
    "    mean_recall = df[\"recall\"].mean()\n",
    "    mean_f1 = df[\"f1\"].mean()\n",
    "\n",
    "    # Calculate MAP and GMAP\n",
    "    map_score = df[\"avg_precision\"].mean()\n",
    "    gmap_score = gmean(df[\"avg_precision\"])\n",
    "\n",
    "    # Create a new dataframe to store the mean scores\n",
    "    mean_scores_df = pd.DataFrame(\n",
    "        {\n",
    "            \"mean_precision\": mean_precision,\n",
    "            \"mean_recall\": mean_recall,\n",
    "            \"mean_f1\": mean_f1,\n",
    "            \"MAP\": map_score,\n",
    "            \"GMAP\": gmap_score,\n",
    "        },\n",
    "        index=[0],\n",
    "    )\n",
    "\n",
    "    # Return both dataframes\n",
    "    return mean_scores_df\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    lda_model, test_corpus, test_df, test_doc_df, dictionary, metric, logger\n",
    "):\n",
    "    # Create a similarity matrix using the trained LDA model\n",
    "    logger.info(\"creating similarity matrix\")\n",
    "    sim_matrix = MatrixSimilarity(\n",
    "        lda_model[test_corpus], num_features=len(dictionary)\n",
    "    )\n",
    "\n",
    "    # get top 10 similar documents for each question\n",
    "    logger.info(\"retrieving top similar documents for each question\")\n",
    "    test_df = test_df.copy()\n",
    "    test_df[\"top10_docs\"] = test_df[\"query_preprocessed\"].apply(\n",
    "        retrieve_doc_indexes, args=(lda_model, sim_matrix, dictionary)\n",
    "    )\n",
    "\n",
    "    test_df[\"top10_pmids\"] = test_df[\"top10_docs\"].apply(\n",
    "        get_pmids_from_doc_indexes, args=(test_doc_df,)\n",
    "    )\n",
    "\n",
    "    # calculate metrics\n",
    "    logger.info(\"calculating metrics\")\n",
    "    eval_df_summary = calculate_metrics(test_df, \"pmids_found\", \"top10_pmids\")\n",
    "    # Return the mean mean_f1 score\n",
    "    score = eval_df_summary[metric].iloc[0]\n",
    "    logger.info(\"%s: %s\", metric, score)\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_max(logs, metric):\n",
    "    df = pd.concat(logs)\n",
    "    print(f\"max {metric}:\")\n",
    "    df = df.sort_values(by=[metric], ascending=False)\n",
    "    # return df[df[metric] == df[metric].max()]\n",
    "    return df.head()\n",
    "\n",
    "# function to filter factoid questions from the dataset\n",
    "def filter_factoid_questions(df):\n",
    "    return df[df[\"type\"] == \"factoid\"]\n",
    "\n",
    "\n",
    "# function to filter list questions from the dataset\n",
    "def filter_list_questions(df):\n",
    "    return df[df[\"type\"] == \"list\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset name:Task10BGoldenEnriched\n",
      "train dataset dir:/workspace/data/raw/bioasq/2022/Task10BGoldenEnriched\n",
      "train doc dir:/workspace/data/processed/bioasq/2022/Task10BGoldenEnriched_documents/\n",
      "test dataset name:Task10BGoldenEnriched\n",
      "test dataset dir:/workspace/data/raw/bioasq/2022/Task10BGoldenEnriched\n",
      "test doc dir:/workspace/data/processed/bioasq/2022/Task10BGoldenEnriched_documents/\n",
      "log dir:/workspace/logs/biomed_qa/answer_extraction/transformer/minilm_ft\n",
      "model dir:/workspace/models/biomed_qa/answer_extraction/transformer/minilm_ft/bioasq/2022/Task10BGoldenEnriched/\n",
      "results dir:/workspace/results/biomed_qa/answer_extraction/transformer/minilm_ft/bioasq/2022/Task10BGoldenEnriched/\n",
      "LOG_FILE: /workspace/logs/biomed_qa/answer_extraction/transformer/minilm_ft/answer_extraction.log\n",
      "Logger initialized\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    # load dir_dict from json file in home directory\n",
    "    home_dir = os.path.expanduser(\"~\")\n",
    "    with open(f\"{home_dir}/.biomedqa_dir.json\", encoding=\"utf-8\") as fp:\n",
    "        dir_dict = json.load(fp)\n",
    "except Exception as exc:\n",
    "    print(\"Error: unable to load directory dictionary. Please run setup.py\")\n",
    "    raise exc\n",
    "\n",
    "# set directories\n",
    "BASE_DIR = dir_dict[\"base_dir\"]\n",
    "DATA_DIR = dir_dict[\"data_dir\"]\n",
    "MODEL_DIR = dir_dict[\"model_dir\"]\n",
    "LOG_DIR = dir_dict[\"log_dir\"]\n",
    "RESULTS_DIR = dir_dict[\"results_dir\"]\n",
    "\n",
    "DATASET = \"bioasq\"\n",
    "YEAR = \"2022\"\n",
    "TRAIN_DATASET_NAME = \"Task10BGoldenEnriched\"\n",
    "TEST_DATASET_NAME = \"Task10BGoldenEnriched\"\n",
    "__file__ = \"notebooks/answer_extraction.ipynb\"\n",
    "# get file directory\n",
    "# FILE_DIR = os.path.dirname(os.path.relpath(__file__))\n",
    "FILE_DIR = \"biomed_qa/answer_extraction/transformer/minilm_ft\"\n",
    "\n",
    "TRAIN_DATASET_DIR = f\"{DATA_DIR}/raw/{DATASET}/{YEAR}/{TRAIN_DATASET_NAME}\"\n",
    "TRAIN_DOC_DIR = (\n",
    "    f\"{DATA_DIR}/processed/{DATASET}/{YEAR}/{TRAIN_DATASET_NAME}_documents/\"\n",
    ")\n",
    "print(f\"train dataset name:{TRAIN_DATASET_NAME}\")\n",
    "print(f\"train dataset dir:{TRAIN_DATASET_DIR}\")\n",
    "print(f\"train doc dir:{TRAIN_DOC_DIR}\")\n",
    "\n",
    "TEST_DATASET_DIR = f\"{DATA_DIR}/raw/{DATASET}/{YEAR}/{TEST_DATASET_NAME}\"\n",
    "TEST_DOC_DIR = (\n",
    "    f\"{DATA_DIR}/processed/{DATASET}/{YEAR}/{TEST_DATASET_NAME}_documents/\"\n",
    ")\n",
    "print(f\"test dataset name:{TEST_DATASET_NAME}\")\n",
    "print(f\"test dataset dir:{TEST_DATASET_DIR}\")\n",
    "print(f\"test doc dir:{TEST_DOC_DIR}\")\n",
    "\n",
    "# set log dir directory according to current file directory\n",
    "LOG_DIR = f\"{LOG_DIR}/{FILE_DIR}\"\n",
    "print(f\"log dir:{LOG_DIR}\")\n",
    "\n",
    "# set model directory according to current file directory\n",
    "MODEL_DIR = f\"{MODEL_DIR}/{FILE_DIR}/{DATASET}/{YEAR}/{TRAIN_DATASET_NAME}/\"\n",
    "print(f\"model dir:{MODEL_DIR}\")\n",
    "\n",
    "# set results directory according to current file directory\n",
    "RESULTS_DIR = f\"{RESULTS_DIR}/{FILE_DIR}/{DATASET}/{YEAR}/{TEST_DATASET_NAME}/\"\n",
    "print(f\"results dir:{RESULTS_DIR}\")\n",
    "\n",
    "# create directories\n",
    "create_directories([LOG_DIR, MODEL_DIR, RESULTS_DIR])\n",
    "\n",
    "# set log file name\n",
    "log_file = os.path.join(\n",
    "    LOG_DIR, os.path.basename(__file__).split(\".\")[0] + \".log\"\n",
    ")\n",
    "print(f\"LOG_FILE: {log_file}\")\n",
    "\n",
    "# initialize logger\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=log_file,\n",
    "    format=\"%(process)d\\t%(asctime)s\\t%(levelname)s\\t%(message)s\",\n",
    "    level=logging.DEBUG,\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "print(\"Logger initialized\")\n",
    "logger.info(\"Logger initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task10BGoldenEnriched - query: 486\n",
      "Task10BGoldenEnriched - unique docs: 3478\n",
      "Task10BGoldenEnriched - queries by type: type\n",
      "factoid    166\n",
      "list        85\n",
      "summary    112\n",
      "yesno      123\n",
      "dtype: int64\n",
      "Task10BGoldenEnriched - query: 486\n",
      "Task10BGoldenEnriched - unique docs: 3478\n",
      "Task10BGoldenEnriched - queries by type: type\n",
      "factoid    166\n",
      "list        85\n",
      "summary    112\n",
      "yesno      123\n",
      "dtype: int64\n",
      "Task10BGoldenEnriched - query: 486\n",
      "Task10BGoldenEnriched - unique docs: 3478\n",
      "Task10BGoldenEnriched - queries by type: type\n",
      "factoid    166\n",
      "list        85\n",
      "summary    112\n",
      "yesno      123\n",
      "dtype: int64\n",
      "num of docs found in corpus:3476\n",
      "Task10BGoldenEnriched - docs per query: 7.292181069958848\n",
      "Task10BGoldenEnriched - query: 486\n",
      "Task10BGoldenEnriched - unique docs: 3478\n",
      "Task10BGoldenEnriched - queries by type: type\n",
      "factoid    166\n",
      "list        85\n",
      "summary    112\n",
      "yesno      123\n",
      "dtype: int64\n",
      "num of docs found in corpus:3476\n",
      "Task10BGoldenEnriched - docs per query: 7.292181069958848\n"
     ]
    }
   ],
   "source": [
    "# load documents\n",
    "logger.info(\"loading documents\")\n",
    "\n",
    "train_doc_df = pd.read_pickle(\n",
    "    f\"{TRAIN_DOC_DIR}{TRAIN_DATASET_NAME}_documents_df.pkl\"\n",
    ")\n",
    "\n",
    "test_doc_df = pd.read_pickle(\n",
    "    f\"{TEST_DOC_DIR}{TEST_DATASET_NAME}_documents_df.pkl\"\n",
    ")\n",
    "\n",
    "# load datasets\n",
    "logger.info(\"loading datasets\")\n",
    "train_df, _ = load_dataset(TRAIN_DATASET_NAME, TRAIN_DATASET_DIR, logger)\n",
    "test_df, _ = load_dataset(TEST_DATASET_NAME, TEST_DATASET_DIR, logger)\n",
    "\n",
    "# prepare datasets\n",
    "train_df = prepare_dataset(\n",
    "    TRAIN_DATASET_NAME, TRAIN_DATASET_DIR, train_doc_df, logger\n",
    ")\n",
    "test_df = prepare_dataset(\n",
    "    TEST_DATASET_NAME, TEST_DATASET_DIR, test_doc_df, logger\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess documents using gensim's preprocess_documents function\n",
    "logger.info(\"preprocessing train documents\")\n",
    "train_doc_df[\"abstractText_preprocessed\"] = preprocess_documents(\n",
    "    train_doc_df[\"abstractText\"]\n",
    ")\n",
    "logger.info(\"preprocessing test documents\")\n",
    "test_doc_df[\"abstractText_preprocessed\"] = preprocess_documents(\n",
    "    test_doc_df[\"abstractText\"]\n",
    ")\n",
    "\n",
    "# Create a dictionary from the preprocessed documents of the training set\n",
    "logger.info(\"creating dictionary\")\n",
    "dictionary = corpora.Dictionary(train_doc_df[\"abstractText_preprocessed\"])\n",
    "\n",
    "# create bag of words corpus of the training set\n",
    "logger.info(\"creating bag of words for train documents\")\n",
    "train_corpus = [\n",
    "    dictionary.doc2bow(text)\n",
    "    for text in train_doc_df[\"abstractText_preprocessed\"]\n",
    "]\n",
    "# Create bag of words corpus of the test set\n",
    "logger.info(\"creating bag of words for test documents\")\n",
    "test_corpus = [\n",
    "    dictionary.doc2bow(text)\n",
    "    for text in test_doc_df[\"abstractText_preprocessed\"]\n",
    "]\n",
    "\n",
    "# preprocess questions\n",
    "logger.info(\"preprocessing test questions\")\n",
    "test_df[\"query_preprocessed\"] = preprocess_documents(\n",
    "    test_df[\"query\"].to_list()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train LDA model\n",
    "lda_model = LdaModel(\n",
    "    corpus=train_corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=1241,\n",
    "    chunksize=2877,\n",
    "    passes=5,\n",
    "    update_every=1,\n",
    "    alpha=\"symmetric\",\n",
    "    eta=\"symmetric\",\n",
    "    decay=0.5,\n",
    "    offset=1,\n",
    "    eval_every=10,\n",
    "    iterations=188,\n",
    "    gamma_threshold=0.001,\n",
    "    minimum_probability=0.01,\n",
    "    random_state=1,\n",
    "    minimum_phi_value=0.01,\n",
    ")\n",
    "# save model\n",
    "logger.info(\"saving model\")\n",
    "LDA_MODEL_DIR = f\"{BASE_DIR}/models/biomed_qa/document_retrieval/{DATASET}/{YEAR}/{TRAIN_DATASET_NAME}\"\n",
    "create_directories([LDA_MODEL_DIR])\n",
    "lda_model.save(f\"{MODEL_DIR}/lda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "logger.info(\"loading model\")\n",
    "LDA_MODEL_DIR = f\"{BASE_DIR}/models/biomed_qa/document_retrieval/{DATASET}/{YEAR}/{TRAIN_DATASET_NAME}\"\n",
    "lda_model = LdaModel.load(f\"{LDA_MODEL_DIR}/lda/lda_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32309417205931074"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(\n",
    "    lda_model, test_corpus, test_df, test_doc_df, dictionary, \"mean_f1\", logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"creating similarity matrix\")\n",
    "sim_matrix = MatrixSimilarity(\n",
    "    lda_model[test_corpus], num_features=len(dictionary)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>num_queries</th>\n",
       "      <th>num_docs</th>\n",
       "      <th>factoid</th>\n",
       "      <th>list</th>\n",
       "      <th>summary</th>\n",
       "      <th>yesno</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10B1_golden</td>\n",
       "      <td>90</td>\n",
       "      <td>588</td>\n",
       "      <td>34</td>\n",
       "      <td>14</td>\n",
       "      <td>19</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10B2_golden</td>\n",
       "      <td>90</td>\n",
       "      <td>621</td>\n",
       "      <td>34</td>\n",
       "      <td>15</td>\n",
       "      <td>23</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10B3_golden</td>\n",
       "      <td>89</td>\n",
       "      <td>553</td>\n",
       "      <td>32</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10B4_golden</td>\n",
       "      <td>90</td>\n",
       "      <td>564</td>\n",
       "      <td>31</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10B5_golden</td>\n",
       "      <td>90</td>\n",
       "      <td>639</td>\n",
       "      <td>29</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10B6_golden</td>\n",
       "      <td>37</td>\n",
       "      <td>547</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset_name  num_queries  num_docs  factoid  list  summary  yesno\n",
       "2  10B1_golden           90       588       34    14       19     23\n",
       "3  10B2_golden           90       621       34    15       23     18\n",
       "1  10B3_golden           89       553       32    11       22     24\n",
       "4  10B4_golden           90       564       31    12       23     24\n",
       "0  10B5_golden           90       639       29    18       15     28\n",
       "5  10B6_golden           37       547        6    15       10      6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>num_queries</th>\n",
       "      <th>num_docs</th>\n",
       "      <th>factoid</th>\n",
       "      <th>list</th>\n",
       "      <th>summary</th>\n",
       "      <th>yesno</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10B1_golden</td>\n",
       "      <td>90</td>\n",
       "      <td>588</td>\n",
       "      <td>34</td>\n",
       "      <td>14</td>\n",
       "      <td>19</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10B2_golden</td>\n",
       "      <td>90</td>\n",
       "      <td>621</td>\n",
       "      <td>34</td>\n",
       "      <td>15</td>\n",
       "      <td>23</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10B3_golden</td>\n",
       "      <td>89</td>\n",
       "      <td>553</td>\n",
       "      <td>32</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10B4_golden</td>\n",
       "      <td>90</td>\n",
       "      <td>564</td>\n",
       "      <td>31</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10B5_golden</td>\n",
       "      <td>90</td>\n",
       "      <td>639</td>\n",
       "      <td>29</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10B6_golden</td>\n",
       "      <td>37</td>\n",
       "      <td>547</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset_name  num_queries  num_docs  factoid  list  summary  yesno\n",
       "2  10B1_golden           90       588       34    14       19     23\n",
       "3  10B2_golden           90       621       34    15       23     18\n",
       "1  10B3_golden           89       553       32    11       22     24\n",
       "4  10B4_golden           90       564       31    12       23     24\n",
       "0  10B5_golden           90       639       29    18       15     28\n",
       "5  10B6_golden           37       547        6    15       10      6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dataset_info(TRAIN_DATASET_DIR))\n",
    "display(dataset_info(TEST_DATASET_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter factoid questions from train_filtered_df\n",
    "train_factoid_df = filter_factoid_questions(train_df)\n",
    "\n",
    "# filter list questions from train_filtered_df\n",
    "train_list_df = filter_list_questions(train_df)\n",
    "\n",
    "# filter factoid questions from test_filtered_df\n",
    "test_factoid_df = filter_factoid_questions(test_df)\n",
    "\n",
    "# filter list questions from test_filtered_df\n",
    "test_list_df = filter_list_questions(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Document\n",
    "\n",
    "def convert_docs_to_haystack_docs(docs):\n",
    "    hs_docs = []\n",
    "    for doc in docs:\n",
    "        document = Document(content=doc)\n",
    "        hs_docs.append(document)\n",
    "    return hs_docs\n",
    "\n",
    "def run_query_and_get_documents_lda(query, doc_df, lda_model, sim_matrix, dictionary, topn=10):\n",
    "    query = preprocess_documents([query])\n",
    "    doc_indexes = retrieve_doc_indexes(query[0], lda_model, sim_matrix, dictionary, topn)\n",
    "    docs = get_text_from_doc_indexes(doc_indexes, doc_df)\n",
    "    return docs\n",
    "\n",
    "def run_query_and_get_documents(query):\n",
    "    pmids = test_df[test_df['query']==query]['pmids'].values\n",
    "    docs = test_doc_df[test_doc_df['pmid'].isin(pmids[0])][\"abstractText\"].values\n",
    "    return docs\n",
    "\n",
    "def run_query_and_get_snippets(query):\n",
    "    snippets = test_df[test_df['query']==query]['snippets'].values\n",
    "    snippets = [snippet['text'] for snippet in snippets[0]]\n",
    "    return snippets\n",
    "\n",
    "def predict(reader,query,context_src,k):\n",
    "    minimal_answers = []\n",
    "    if context_src == 'by_documents':\n",
    "        docs = run_query_and_get_documents(query)\n",
    "    elif context_src == 'by_snippets':\n",
    "        docs = run_query_and_get_snippets(query)\n",
    "    elif context_src == 'by_lda':\n",
    "        docs = run_query_and_get_documents_lda(query, test_doc_df, lda_model, sim_matrix, dictionary)\n",
    "    else:\n",
    "        # raise error\n",
    "        print('context_src must be one of the following: by_documents, by_snippets, by_lda')        \n",
    "    docs = convert_docs_to_haystack_docs(docs)\n",
    "    answers = reader.predict(query=query, documents=docs, top_k=k)\n",
    "    [ minimal_answers.append([ans.answer]) for ans in answers['answers'] if [ans.answer] not in minimal_answers]\n",
    "    return minimal_answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating factoid questions\n",
    "def manual_evaluate_factoid(DATA_DIR, MODEL_DIR, RESULTS_DIR, context_src):\n",
    "    from haystack.nodes import FARMReader\n",
    "    reader = FARMReader(model_name_or_path=MODEL_DIR, use_gpu=True)\n",
    "    model_name = MODEL_DIR.split(\"/\")[-1]\n",
    "    RESULTS_DIR = f\"{RESULTS_DIR}/{model_name}/{context_src}\"\n",
    "    create_directories([RESULTS_DIR])\n",
    "    for json_file in glob.glob(os.path.join(DATA_DIR, '*.json')):\n",
    "        print(json_file)\n",
    "        with open(json_file) as fp:\n",
    "            json_data = ''.join(fp)\n",
    "\n",
    "        data = json.loads(json_data)\n",
    "        data = data[\"questions\"]\n",
    "        df = pd.DataFrame(data)\n",
    "        factoid_df = df[df['type'] == 'factoid']\n",
    "        factoid_df['predicted_answers'] = factoid_df.apply( lambda x: predict(reader, x.body, context_src, 5), axis = 1 )\n",
    "        # factoid_df['predicted_answers'] =  0\n",
    "        factoid_df['predicted_answer_pos'] = \"?\"\n",
    "        file_name = json_file.replace(DATA_DIR,'')[1:-5]+\"_factoid.csv\"\n",
    "        factoid_df = factoid_df[['body','type','exact_answer','predicted_answers','predicted_answer_pos']]\n",
    "        csv_file = os.path.join(RESULTS_DIR,file_name)\n",
    "        factoid_df.to_csv(csv_file, index=False)\n",
    "        print(f\"Saved {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual_evaluate_factoid(TEST_DATASET_DIR, \"models/factoid_model_5\", RESULTS_DIR, \"by_snippets\")\n",
    "manual_evaluate_factoid(TEST_DATASET_DIR, \"models/factoid_model_5\", RESULTS_DIR, \"by_documents\")\n",
    "manual_evaluate_factoid(TEST_DATASET_DIR, \"models/factoid_model_5\", RESULTS_DIR, \"by_lda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "def calculate_strict_accuracy(predictions):\n",
    "    \"\"\"\n",
    "    Calculates the strict accuracy.\n",
    "\n",
    "    Args:\n",
    "        predictions (Series): Series of predicted answer positions.\n",
    "\n",
    "    Returns:\n",
    "        float: Strict accuracy.\n",
    "    \"\"\"\n",
    "    correct_predictions = predictions[predictions == 1].count()\n",
    "    total_predictions = len(predictions)\n",
    "\n",
    "    return correct_predictions / total_predictions\n",
    "\n",
    "def calculate_lenient_accuracy(predictions):\n",
    "    \"\"\"\n",
    "    Calculates the lenient accuracy of 5.\n",
    "\n",
    "    Args:\n",
    "        predictions (Series): Series of predicted answer positions.\n",
    "\n",
    "    Returns:\n",
    "        float: Lenient accuracy of 5.\n",
    "    \"\"\"\n",
    "    correct_predictions = predictions[predictions > 0].count()\n",
    "    total_predictions = len(predictions)\n",
    "\n",
    "    return correct_predictions / total_predictions\n",
    "\n",
    "def calculate_mrr(predictions):\n",
    "    \"\"\"\n",
    "    Calculates the Mean Reciprocal Rank (MRR).\n",
    "\n",
    "    Args:\n",
    "        predictions (Series): Series of predicted answer positions.\n",
    "\n",
    "    Returns:\n",
    "        float: Mean Reciprocal Rank (MRR).\n",
    "    \"\"\"\n",
    "    total_reciprocal_ranks = (1 / predictions[predictions > 0]).sum()\n",
    "    total_predictions = len(predictions)\n",
    "\n",
    "    return total_reciprocal_ranks / total_predictions\n",
    "\n",
    "def calculate_metrics_factoid(dir):\n",
    "    # Find CSV files that end with \"manual_eval.csv\"\n",
    "    filenames = glob.glob(os.path.join(dir , \"*factoid_manual_eval.csv\"))\n",
    "\n",
    "    # dictionary to store the results\n",
    "    results = {}\n",
    "\n",
    "    for filename in filenames:\n",
    "        print(filename)\n",
    "        df = pd.read_csv(filename)\n",
    "\n",
    "        # Extract predicted answer positions\n",
    "        predicted_answer_positions = df['predicted_answer_pos']\n",
    "        print(predicted_answer_positions)\n",
    "\n",
    "        # Calculate metrics\n",
    "        strict_accuracy = calculate_strict_accuracy(predicted_answer_positions)\n",
    "        lenient_accuracy = calculate_lenient_accuracy(predicted_answer_positions)\n",
    "        mrr = calculate_mrr(predicted_answer_positions)\n",
    "\n",
    "        # Store results\n",
    "        batch = int(os.path.basename(filename)[3:4])\n",
    "        results[batch] = [strict_accuracy, lenient_accuracy, mrr]\n",
    "\n",
    "    # Create DataFrame from results sorted by filename\n",
    "    results_df = pd.DataFrame.from_dict(results, orient='index', columns=['Strict', 'Lenient', 'MRR']).sort_index()\n",
    "    # set index as  1st column\n",
    "    results_df.index.name = 'batch'\n",
    "    results_df.reset_index(inplace=True)\n",
    "    # save results to csv\n",
    "    results_df.to_csv(os.path.join(dir, f\"{TEST_DATASET_NAME}_factoid_results.csv\"), index=False)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/results/biomed_qa/answer_extraction/transformer/minilm_ft/bioasq/2022/Task10BGoldenEnriched//factoid_model_5/by_documents/10B3_golden_factoid_manual_eval.csv\n",
      "0     3\n",
      "1     1\n",
      "2     1\n",
      "3     4\n",
      "4     2\n",
      "5     1\n",
      "6     2\n",
      "7     1\n",
      "8     1\n",
      "9     1\n",
      "10    1\n",
      "11    3\n",
      "12    2\n",
      "13    1\n",
      "14    1\n",
      "15    1\n",
      "16    2\n",
      "17    2\n",
      "18    1\n",
      "19    1\n",
      "20    1\n",
      "21    1\n",
      "22    1\n",
      "23    1\n",
      "24    3\n",
      "25    1\n",
      "26    3\n",
      "27    0\n",
      "28    1\n",
      "29    1\n",
      "30    1\n",
      "31    1\n",
      "Name: predicted_answer_pos, dtype: int64\n",
      "/workspace/results/biomed_qa/answer_extraction/transformer/minilm_ft/bioasq/2022/Task10BGoldenEnriched//factoid_model_5/by_documents/10B4_golden_factoid_manual_eval.csv\n",
      "0     5\n",
      "1     2\n",
      "2     3\n",
      "3     1\n",
      "4     2\n",
      "5     1\n",
      "6     5\n",
      "7     0\n",
      "8     0\n",
      "9     2\n",
      "10    1\n",
      "11    1\n",
      "12    0\n",
      "13    4\n",
      "14    1\n",
      "15    1\n",
      "16    1\n",
      "17    1\n",
      "18    1\n",
      "19    1\n",
      "20    1\n",
      "21    1\n",
      "22    2\n",
      "23    2\n",
      "24    2\n",
      "25    1\n",
      "26    1\n",
      "27    4\n",
      "28    1\n",
      "29    3\n",
      "30    1\n",
      "Name: predicted_answer_pos, dtype: int64\n",
      "/workspace/results/biomed_qa/answer_extraction/transformer/minilm_ft/bioasq/2022/Task10BGoldenEnriched//factoid_model_5/by_documents/10B5_golden_factoid_manual_eval.csv\n",
      "0     1\n",
      "1     1\n",
      "2     1\n",
      "3     1\n",
      "4     4\n",
      "5     1\n",
      "6     0\n",
      "7     0\n",
      "8     1\n",
      "9     1\n",
      "10    1\n",
      "11    1\n",
      "12    3\n",
      "13    1\n",
      "14    1\n",
      "15    3\n",
      "16    1\n",
      "17    2\n",
      "18    5\n",
      "19    1\n",
      "20    2\n",
      "21    1\n",
      "22    5\n",
      "23    1\n",
      "24    1\n",
      "25    0\n",
      "26    1\n",
      "27    2\n",
      "28    0\n",
      "Name: predicted_answer_pos, dtype: int64\n",
      "/workspace/results/biomed_qa/answer_extraction/transformer/minilm_ft/bioasq/2022/Task10BGoldenEnriched//factoid_model_5/by_documents/10B2_golden_factoid_manual_eval.csv\n",
      "0     1\n",
      "1     1\n",
      "2     1\n",
      "3     4\n",
      "4     0\n",
      "5     1\n",
      "6     1\n",
      "7     1\n",
      "8     1\n",
      "9     0\n",
      "10    1\n",
      "11    0\n",
      "12    1\n",
      "13    1\n",
      "14    1\n",
      "15    4\n",
      "16    1\n",
      "17    1\n",
      "18    1\n",
      "19    1\n",
      "20    5\n",
      "21    1\n",
      "22    1\n",
      "23    1\n",
      "24    1\n",
      "25    1\n",
      "26    1\n",
      "27    1\n",
      "28    3\n",
      "29    1\n",
      "30    1\n",
      "31    0\n",
      "32    0\n",
      "33    1\n",
      "Name: predicted_answer_pos, dtype: int64\n",
      "/workspace/results/biomed_qa/answer_extraction/transformer/minilm_ft/bioasq/2022/Task10BGoldenEnriched//factoid_model_5/by_documents/10B6_golden_factoid_manual_eval.csv\n",
      "0    0\n",
      "1    1\n",
      "2    0\n",
      "3    1\n",
      "4    1\n",
      "5    1\n",
      "Name: predicted_answer_pos, dtype: int64\n",
      "/workspace/results/biomed_qa/answer_extraction/transformer/minilm_ft/bioasq/2022/Task10BGoldenEnriched//factoid_model_5/by_documents/10B1_golden_factoid_manual_eval.csv\n",
      "0     2\n",
      "1     1\n",
      "2     1\n",
      "3     5\n",
      "4     1\n",
      "5     0\n",
      "6     4\n",
      "7     3\n",
      "8     2\n",
      "9     0\n",
      "10    1\n",
      "11    0\n",
      "12    1\n",
      "13    3\n",
      "14    4\n",
      "15    3\n",
      "16    1\n",
      "17    1\n",
      "18    2\n",
      "19    0\n",
      "20    2\n",
      "21    0\n",
      "22    1\n",
      "23    1\n",
      "24    1\n",
      "25    1\n",
      "26    1\n",
      "27    1\n",
      "28    3\n",
      "29    3\n",
      "30    1\n",
      "31    0\n",
      "32    0\n",
      "33    1\n",
      "Name: predicted_answer_pos, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch</th>\n",
       "      <th>Strict</th>\n",
       "      <th>Lenient</th>\n",
       "      <th>MRR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.441176</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.569608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.765686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.783854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.663441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.683333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   batch    Strict   Lenient       MRR\n",
       "0      1  0.441176  0.794118  0.569608\n",
       "1      2  0.735294  0.852941  0.765686\n",
       "2      3  0.656250  0.968750  0.783854\n",
       "3      4  0.516129  0.903226  0.663441\n",
       "4      5  0.586207  0.862069  0.683333\n",
       "5      6  0.666667  0.666667  0.666667"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'factoid_model_5'\n",
    "# calculate_metrics_factoid(f\"{RESULTS_DIR}/{model_name}/by_snippets\")\n",
    "calculate_metrics_factoid(f\"{RESULTS_DIR}/{model_name}/by_documents\")\n",
    "# calculate_metrics_factoid(f\"{RESULTS_DIR}/{model_name}/by_lda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating factoid questions\n",
    "def manual_evaluate_list(DATA_DIR, MODEL_DIR, RESULTS_DIR, context_src):\n",
    "    from haystack.nodes import FARMReader\n",
    "    reader = FARMReader(model_name_or_path=MODEL_DIR, use_gpu=True)\n",
    "    model_name = MODEL_DIR.split(\"/\")[-1]\n",
    "    RESULTS_DIR = f\"{RESULTS_DIR}/{model_name}/{context_src}\"\n",
    "    create_directories([RESULTS_DIR])\n",
    "    for json_file in glob.glob(os.path.join(DATA_DIR, '*.json')):\n",
    "        print(json_file)\n",
    "        with open(json_file) as fp:\n",
    "            json_data = ''.join(fp)\n",
    "\n",
    "        data = json.loads(json_data)\n",
    "        data = data[\"questions\"]\n",
    "        df = pd.DataFrame(data)\n",
    "        list_df = df[df['type'] == 'list']\n",
    "        list_df['predicted_answers'] = list_df.apply( lambda x: predict(reader, x.body, context_src, 10), axis = 1 )\n",
    "        list_df['TP'] = \"?\"\n",
    "        list_df['FP'] = \"?\"\n",
    "        list_df['FN'] = \"?\"\n",
    "        file_name = json_file.replace(DATA_DIR,'')[1:-5]+\"_list.csv\"\n",
    "        list_df = list_df[['body','type','exact_answer','predicted_answers','TP','FP','FN']]\n",
    "        list_df.to_csv(os.path.join(RESULTS_DIR,file_name), index=False)\n",
    "        print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual_evaluate_list(TEST_DATASET_DIR, \"models/list_model_15\", RESULTS_DIR, \"by_snippets\")\n",
    "manual_evaluate_list(TEST_DATASET_DIR, \"models/list_model_15\", RESULTS_DIR, \"by_documents\")\n",
    "manual_evaluate_list(TEST_DATASET_DIR, \"models/list_model_15\", RESULTS_DIR, \"by_lda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_list(dir):\n",
    "    file_names = glob.glob(os.path.join( dir, \"*list_manual_eval.csv\"))\n",
    "\n",
    "    # dictionary to store the results\n",
    "    results = {}\n",
    "\n",
    "    # iterate through each csv file\n",
    "    for file_name in file_names:\n",
    "        # read csv file into pandas dataframe\n",
    "        df = pd.read_csv(file_name)\n",
    "        # calculate precision, recall and f-measure for each row\n",
    "        df['precision'] = df['TP'] / (df['TP'] + df['FP'])\n",
    "        df['recall'] = df['TP'] / (df['TP'] + df['FN'])\n",
    "        df['f_measure'] = 2 * df['precision'] * df['recall'] / (df['precision'] + df['recall'])\n",
    "        \n",
    "        # calculate mean precision, recall and f-measure for the file\n",
    "        mean_precision = df['precision'].mean()\n",
    "        mean_recall = df['recall'].mean()\n",
    "        mean_f_measure = df['f_measure'].mean()\n",
    "        # Store results\n",
    "        batch = os.path.basename(file_name)[3:4]\n",
    "        results[batch] = [mean_precision, mean_recall, mean_f_measure]\n",
    "\n",
    "    # Create DataFrame from results sorted by filename\n",
    "    results_df = pd.DataFrame.from_dict(results, orient='index', columns=['Mean Precision','Mean Recall','Mean F-Measure']).sort_index()\n",
    "    # set index as  1st column\n",
    "    results_df.index.name = 'batch'\n",
    "    results_df.reset_index(inplace=True)\n",
    "    # save results to csv\n",
    "    results_df.to_csv(os.path.join(dir, f\"{TEST_DATASET_NAME}_list_results.csv\"), index=False)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch</th>\n",
       "      <th>Mean Precision</th>\n",
       "      <th>Mean Recall</th>\n",
       "      <th>Mean F-Measure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.527183</td>\n",
       "      <td>0.713492</td>\n",
       "      <td>0.559418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.449339</td>\n",
       "      <td>0.592963</td>\n",
       "      <td>0.514640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.363095</td>\n",
       "      <td>0.632468</td>\n",
       "      <td>0.478786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.504167</td>\n",
       "      <td>0.484325</td>\n",
       "      <td>0.400337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.465929</td>\n",
       "      <td>0.623292</td>\n",
       "      <td>0.574664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.415204</td>\n",
       "      <td>0.523118</td>\n",
       "      <td>0.458029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  batch  Mean Precision  Mean Recall  Mean F-Measure\n",
       "0     1        0.527183     0.713492        0.559418\n",
       "1     2        0.449339     0.592963        0.514640\n",
       "2     3        0.363095     0.632468        0.478786\n",
       "3     4        0.504167     0.484325        0.400337\n",
       "4     5        0.465929     0.623292        0.574664\n",
       "5     6        0.415204     0.523118        0.458029"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'list_model_15'\n",
    "# calculate_metrics_list(f\"{RESULTS_DIR}/{model_name}/by_snippets\")\n",
    "calculate_metrics_list(f\"{RESULTS_DIR}/{model_name}/by_documents\")\n",
    "# calculate_metrics_list(f\"{RESULTS_DIR}/{model_name}/by_lda\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
