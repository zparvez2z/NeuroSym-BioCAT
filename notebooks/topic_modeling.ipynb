{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import json\n",
    "import pickle\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    average_precision_score,\n",
    ")\n",
    "from scipy.stats import gmean\n",
    "\n",
    "# import gensim libraries\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "from gensim.parsing.preprocessing import preprocess_documents\n",
    "from gensim.models.callbacks import Metric\n",
    "\n",
    "import optuna\n",
    "from optuna.storages import RetryFailedTrialCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directories(directories):\n",
    "    for directory in directories:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "            print(f\"Created directory: {directory}\")\n",
    "\n",
    "\n",
    "def prepare_dataset(dataset_name, dataset_dir, doc_df):\n",
    "    dfs = []\n",
    "    for json_file in glob.glob(os.path.join(dataset_dir, \"*.json\")):\n",
    "        with open(json_file) as fp:\n",
    "            json_data = \"\".join(fp)\n",
    "        data = json.loads(json_data)\n",
    "        data = data[\"questions\"]\n",
    "        dfs.append(pd.DataFrame(data))\n",
    "    df = pd.concat(dfs)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    def get_pmids(document):\n",
    "        return [re.findall(r\"/\\d+\", link)[0][1:] for link in document]\n",
    "\n",
    "    df[\"pmids\"] = df[\"documents\"].apply(get_pmids)\n",
    "    pmids = list(chain.from_iterable(df[\"pmids\"].to_list()))\n",
    "    print(\n",
    "        f\"total number of unique docs provided in {dataset_name}: {len(set(pmids))}\"\n",
    "    )\n",
    "    logger.info(\n",
    "        \"total number of unique docs provided in %s: %d\",\n",
    "        dataset_name,\n",
    "        len(set(pmids)),\n",
    "    )\n",
    "\n",
    "    corpus_df = doc_df[doc_df[\"pmid\"].isin(pmids)]\n",
    "    print(f\"num of docs found in corpus:{corpus_df.shape[0]}\")\n",
    "    logger.info(\"num of docs found in corpus: %d\", corpus_df.shape[0])\n",
    "\n",
    "    def filter_pmid(pmids):\n",
    "        filtered_pmids = [\n",
    "            pmid for pmid in pmids if pmid in doc_df[\"pmid\"].to_list()\n",
    "        ]\n",
    "        return filtered_pmids\n",
    "\n",
    "    df[\"pmids_found\"] = df[\"pmids\"].apply(filter_pmid)\n",
    "\n",
    "    filtered_df = df[df[\"pmids_found\"].apply(len) > 0]\n",
    "    # average number of docs per query\n",
    "    total_num_docs = sum(filtered_df[\"pmids_found\"].apply(len))\n",
    "    total_num_queries = filtered_df[\"body\"].shape[0]\n",
    "    avg_num_docs_per_query = total_num_docs / total_num_queries\n",
    "    print(f\"avg num of docs per query: {avg_num_docs_per_query}\")\n",
    "    logger.info(\"avg num of docs per query: %d\", avg_num_docs_per_query)\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "# retrieve the top N similar documents for a given document or query\n",
    "def retrieve_documents(query, lda_model, sim_matrix, topn=10):\n",
    "    vec_bow = dictionary.doc2bow(query)\n",
    "    vec_lda = lda_model[vec_bow]\n",
    "    sims = sim_matrix[vec_lda]\n",
    "    sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "    return sims[:topn]\n",
    "\n",
    "\n",
    "# get pmids from doc indexes\n",
    "def get_pmids_from_doc_indexes(doc_indexes, doc_df):\n",
    "    return [doc_df[\"pmid\"].iloc[doc_idx[0]] for doc_idx in doc_indexes]\n",
    "    \n",
    "\n",
    "def calculate_metrics(df, pred_col_name):\n",
    "    golden = []\n",
    "    predicted = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        golden.append({\n",
    "            \"id\": df.iloc[i][\"id\"],\n",
    "            \"body\": df.iloc[i][\"body\"],\n",
    "            \"type\": df.iloc[i][\"type\"],\n",
    "            \"documents\": df.iloc[i][\"documents\"],\n",
    "            \"snippets\": [],\n",
    "            \"ideal_answer\": [],\n",
    "            \"exact_answer\": [],\n",
    "        })\n",
    "\n",
    "        predicted.append({\n",
    "            \"id\": df.iloc[i][\"id\"],\n",
    "            \"body\": df.iloc[i][\"body\"],\n",
    "            \"type\": df.iloc[i][\"type\"],\n",
    "            \"documents\": df.iloc[i][pred_col_name],\n",
    "            \"snippets\": [],\n",
    "            \"ideal_answer\": [],\n",
    "            \"exact_answer\": [],\n",
    "        })\n",
    "\n",
    "    with open(\"golden.json\", \"w\") as fp:\n",
    "        json.dump({\"questions\": golden}, fp, indent=4)\n",
    "\n",
    "    with open(\"predicted.json\", \"w\") as fp:\n",
    "        json.dump({\"questions\": predicted}, fp, indent=4)\n",
    "\n",
    "    evaluation_command = 'java -Xmx10G -cp \"/workspaces/biomed_qa-zparvez2z/Evaluation-Measures/flat/BioASQEvaluation/dist/*\" evaluation.EvaluatorTask1b -phaseA -e 5 golden.json predicted.json -verbose'\n",
    "    evaluation_output = os.popen(evaluation_command).read()\n",
    "    evaluation_output = evaluation_output.strip().split(\"\\n\")\n",
    "    score_dict = {}\n",
    "    for line in evaluation_output[1:]:\n",
    "            metric, score = line.split(\":\")\n",
    "            score_dict[metric.strip()] = float(score.strip())\n",
    "\n",
    "    return pd.DataFrame(score_dict, index=[0])\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    lda_model, test_corpus, test_df, test_doc_df, dictionary, metric, logger\n",
    "):\n",
    "    # Create a similarity matrix using the trained LDA model\n",
    "    logger.info(\"creating similarity matrix\")\n",
    "    sim_matrix = MatrixSimilarity(\n",
    "        lda_model[test_corpus], num_features=len(dictionary)\n",
    "    )\n",
    "\n",
    "    # get top 10 similar documents for each question\n",
    "    logger.info(\"retrieving top similar documents for each question\")\n",
    "    test_df = test_df.copy()\n",
    "    test_df[\"top10_sims\"] = test_df[\"body_preprocessed\"].apply(\n",
    "        retrieve_documents, args=(lda_model, sim_matrix)\n",
    "    )\n",
    "\n",
    "    test_df[\"top10_pmids\"] = test_df[\"top10_sims\"].apply(\n",
    "        get_pmids_from_doc_indexes, args=(test_doc_df,)\n",
    "    )\n",
    "\n",
    "    test_df[\"top10_docs\"] = test_df[\"top10_pmids\"].apply(\n",
    "        lambda docs: [\"http://www.ncbi.nlm.nih.gov/pubmed/\" + str(pmid) for pmid in docs]\n",
    "    )\n",
    "    # calculate metrics\n",
    "    logger.info(\"calculating metrics\")\n",
    "    eval_df_summary = calculate_metrics(test_df, \"top10_docs\")\n",
    "    # Return the metric score\n",
    "    score = eval_df_summary[metric].iloc[0]       \n",
    "    logger.info(\"%s: %s\", metric, score)\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_max(logs, metric):\n",
    "    df = pd.concat(logs)\n",
    "    print(f\"max {metric}:\")\n",
    "    df = df.sort_values(by=[metric], ascending=False)\n",
    "    # return df[df[metric] == df[metric].max()]\n",
    "    return df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# init directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset name:Task10BGoldenEnriched\n",
      "train dataset dir:/workspaces/biomed_qa-zparvez2z/data/raw/bioasq/2022/Task10BGoldenEnriched\n",
      "train doc dir:/workspaces/biomed_qa-zparvez2z/data/processed/bioasq/2022/Task10BGoldenEnriched_documents/\n",
      "test dataset name:Task10BGoldenEnriched\n",
      "test dataset dir:/workspaces/biomed_qa-zparvez2z/data/raw/bioasq/2022/Task10BGoldenEnriched\n",
      "test doc dir:/workspaces/biomed_qa-zparvez2z/data/processed/bioasq/2022/Task10BGoldenEnriched_documents/\n",
      "log dir:/workspaces/biomed_qa-zparvez2z/logs/notebooks\n",
      "model dir:/workspaces/biomed_qa-zparvez2z/models/notebooks/bioasq/2022/Task10BGoldenEnriched/\n",
      "results dir:/workspaces/biomed_qa-zparvez2z/results/notebooks/bioasq/2022/Task10BGoldenEnriched/\n",
      "LOG_FILE: /workspaces/biomed_qa-zparvez2z/logs/notebooks/topic_modeling.log\n",
      "total number of unique docs provided in Task10BGoldenEnriched: 3478\n",
      "num of docs found in corpus:3476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg num of docs per query: 7.292181069958848\n",
      "total number of unique docs provided in Task10BGoldenEnriched: 3478\n",
      "num of docs found in corpus:3476\n",
      "avg num of docs per query: 7.292181069958848\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    # load dir_dict from json file in home directory\n",
    "    home_dir = os.path.expanduser(\"~\")\n",
    "    with open(f\"{home_dir}/.biomedqa_dir.json\", encoding=\"utf-8\") as fp:\n",
    "        dir_dict = json.load(fp)\n",
    "except Exception as exc:\n",
    "    print(\"Error: unable to load directory dictionary. Please run setup.py\")\n",
    "    raise exc\n",
    "\n",
    "# set directories\n",
    "BASE_DIR = dir_dict[\"base_dir\"]\n",
    "DATA_DIR = dir_dict[\"data_dir\"]\n",
    "MODEL_DIR = dir_dict[\"model_dir\"]\n",
    "LOG_DIR = dir_dict[\"log_dir\"]\n",
    "RESULTS_DIR = dir_dict[\"results_dir\"]\n",
    "\n",
    "DATASET = \"bioasq\"\n",
    "YEAR = \"2022\"\n",
    "__file__ = \"notebooks/topic_modeling.ipynb\"\n",
    "\n",
    "TRAIN_DATASET_NAME = \"Task10BGoldenEnriched\"\n",
    "TRAIN_DATASET_DIR = f\"{DATA_DIR}/raw/{DATASET}/{YEAR}/{TRAIN_DATASET_NAME}\"\n",
    "TRAIN_DOC_DIR = (\n",
    "    f\"{DATA_DIR}/processed/{DATASET}/{YEAR}/{TRAIN_DATASET_NAME}_documents/\"\n",
    ")\n",
    "print(f\"train dataset name:{TRAIN_DATASET_NAME}\")\n",
    "print(f\"train dataset dir:{TRAIN_DATASET_DIR}\")\n",
    "print(f\"train doc dir:{TRAIN_DOC_DIR}\")\n",
    "\n",
    "TEST_DATASET_NAME = \"Task10BGoldenEnriched\"\n",
    "TEST_DATASET_DIR = f\"{DATA_DIR}/raw/{DATASET}/{YEAR}/{TEST_DATASET_NAME}\"\n",
    "TEST_DOC_DIR = (\n",
    "    f\"{DATA_DIR}/processed/{DATASET}/{YEAR}/{TEST_DATASET_NAME}_documents/\"\n",
    ")\n",
    "print(f\"test dataset name:{TEST_DATASET_NAME}\")\n",
    "print(f\"test dataset dir:{TEST_DATASET_DIR}\")\n",
    "print(f\"test doc dir:{TEST_DOC_DIR}\")\n",
    "\n",
    "# get file directory\n",
    "FILE_DIR = os.path.dirname(os.path.relpath(__file__))\n",
    "\n",
    "# set log dir directory according to current file directory\n",
    "LOG_DIR = f\"{LOG_DIR}/{FILE_DIR}\"\n",
    "print(f\"log dir:{LOG_DIR}\")\n",
    "\n",
    "# set model directory according to current file directory\n",
    "MODEL_DIR = f\"{MODEL_DIR}/{FILE_DIR}/{DATASET}/{YEAR}/{TRAIN_DATASET_NAME}/\"\n",
    "print(f\"model dir:{MODEL_DIR}\")\n",
    "\n",
    "# set results directory according to current file directory\n",
    "RESULTS_DIR = f\"{RESULTS_DIR}/{FILE_DIR}/{DATASET}/{YEAR}/{TRAIN_DATASET_NAME}/\"\n",
    "print(f\"results dir:{RESULTS_DIR}\")\n",
    "\n",
    "# create directories\n",
    "create_directories([LOG_DIR, MODEL_DIR, RESULTS_DIR])\n",
    "\n",
    "# set log file name\n",
    "log_file = os.path.join(\n",
    "    LOG_DIR, os.path.basename(__file__).split(\".\")[0] + \".log\"\n",
    ")\n",
    "print(f\"LOG_FILE: {log_file}\")\n",
    "\n",
    "# initialize logger\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=log_file,\n",
    "    format=\"%(process)d\\t%(asctime)s\\t%(levelname)s\\t%(message)s\",\n",
    "    level=logging.DEBUG,\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"Logger initialized\")\n",
    "\n",
    "# load documents\n",
    "logger.info(\"loading documents\")\n",
    "train_doc_df = pd.read_pickle(\n",
    "    f\"{TRAIN_DOC_DIR}{TRAIN_DATASET_NAME}_documents_df.pkl\"\n",
    ")\n",
    "\n",
    "test_doc_df = pd.read_pickle(\n",
    "    f\"{TEST_DOC_DIR}{TEST_DATASET_NAME}_documents_df.pkl\"\n",
    ")\n",
    "\n",
    "train_filtered_df = prepare_dataset(\n",
    "    TRAIN_DATASET_NAME, TRAIN_DATASET_DIR, train_doc_df\n",
    ")\n",
    "test_filtered_df = prepare_dataset(\n",
    "    TEST_DATASET_NAME, TEST_DATASET_DIR, test_doc_df\n",
    ")\n",
    "\n",
    "# preprocess documents using gensim's preprocess_documents function\n",
    "logger.info(\"preprocessing train documents\")\n",
    "train_doc_df[\"abstractText_preprocessed\"] = preprocess_documents(\n",
    "    train_doc_df[\"abstractText\"]\n",
    ")\n",
    "logger.info(\"preprocessing test documents\")\n",
    "test_doc_df[\"abstractText_preprocessed\"] = preprocess_documents(\n",
    "    test_doc_df[\"abstractText\"]\n",
    ")\n",
    "\n",
    "# Create a dictionary from the preprocessed documents of the training set\n",
    "logger.info(\"creating dictionary\")\n",
    "dictionary = corpora.Dictionary(train_doc_df[\"abstractText_preprocessed\"])\n",
    "\n",
    "# create bag of words corpus of the training set\n",
    "logger.info(\"creating bag of words for train documents\")\n",
    "train_corpus = [\n",
    "    dictionary.doc2bow(text)\n",
    "    for text in train_doc_df[\"abstractText_preprocessed\"]\n",
    "]\n",
    "# Create bag of words corpus of the test set\n",
    "logger.info(\"creating bag of words for test documents\")\n",
    "test_corpus = [\n",
    "    dictionary.doc2bow(text)\n",
    "    for text in test_doc_df[\"abstractText_preprocessed\"]\n",
    "]\n",
    "\n",
    "# preprocess questions\n",
    "logger.info(\"preprocessing test questions\")\n",
    "test_filtered_df[\"body_preprocessed\"] = preprocess_documents(\n",
    "    test_filtered_df[\"body\"].to_list()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define custom callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom gensim callback to store intermediate scores and add the best result to the optuna study\n",
    "class IntermediateScoreCallback(Metric):\n",
    "    \"\"\"Callback to log information about training\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        test_corpus,\n",
    "        test_df,\n",
    "        test_doc_df,\n",
    "        dictionary,\n",
    "        trial,\n",
    "        logger=None,\n",
    "        title=None,\n",
    "    ):\n",
    "        self.logger = logger\n",
    "        self.title = title\n",
    "        self.pass_count = 0\n",
    "        self.test_corpus = test_corpus\n",
    "        self.test_df = test_df\n",
    "        self.test_doc_df = test_doc_df\n",
    "        self.dictionary = dictionary\n",
    "        self.trial = trial\n",
    "        self.datetime_start = datetime.datetime.now()\n",
    "        self.score = None\n",
    "\n",
    "    def get_value(self, **kwargs):\n",
    "        super(IntermediateScoreCallback, self).set_parameters(**kwargs)\n",
    "        self.pass_count += 1\n",
    "        # train_and_evaluate(self,test_dataset_name,test_filtered_df,test_doc_df, **kwargs)\n",
    "        self.score = evaluate(\n",
    "            lda_model=self.model,\n",
    "            test_corpus=self.test_corpus,\n",
    "            test_df=self.test_df,\n",
    "            test_doc_df=self.test_doc_df,\n",
    "            dictionary=self.dictionary,\n",
    "            metric=optimization_metric,\n",
    "            logger=self.logger,\n",
    "        )\n",
    "        # add intermediate score to optuna study with start and end time\n",
    "        intermediate_trial = optuna.trial.create_trial(\n",
    "            params=self.trial.params,\n",
    "            distributions=self.trial.distributions,\n",
    "            value=self.score,\n",
    "            user_attrs=None,\n",
    "            system_attrs=None,\n",
    "            state=optuna.trial.TrialState.COMPLETE,\n",
    "        )\n",
    "        intermediate_trial.params[\"passes\"] = self.pass_count\n",
    "        intermediate_trial.datetime_start = self.datetime_start\n",
    "        intermediate_trial.datetime_complete = datetime.datetime.now()\n",
    "        study.add_trial(intermediate_trial)\n",
    "\n",
    "\n",
    "# Define the objective function\n",
    "def objective(\n",
    "    trial,\n",
    "    train_corpus=train_corpus,\n",
    "    test_corpus=test_corpus,\n",
    "    test_filtered_df=test_filtered_df,\n",
    "    test_doc_df=test_doc_df,\n",
    "    dictionary=dictionary,\n",
    "    logger=logger,\n",
    "):\n",
    "    # Define the hyperparameters to optimize\n",
    "    num_topics = trial.suggest_int(\"num_topics\", 10, 100)\n",
    "    chunksize = trial.suggest_int(\"chunksize\", 1, len(train_corpus))\n",
    "    passes = trial.suggest_int(\"passes\", 1, 50)\n",
    "    update_every = 0\n",
    "    alpha = \"symmetric\"\n",
    "    eta = \"symmetric\"\n",
    "    decay = 0.5\n",
    "    offset = 1\n",
    "    eval_every = 10\n",
    "    iterations = trial.suggest_int(\"iterations\", 1, 100)\n",
    "    gamma_threshold = 0.001\n",
    "    minimum_probability = 0.01\n",
    "    random_state = 1\n",
    "    minimum_phi_value = 0.01\n",
    "\n",
    "    # Train the LDA model with the suggested hyperparameters\n",
    "    logger.info(\n",
    "        \"starting training lda model with parameters: num_topics:%s, chunksize:%s, passes:%s, update_every:%s, alpha:%s, eta:%s, decay:%s, offset:%s, eval_every:%s, iterations:%s, gamma_threshold:%s, minimum_probability:%s, random_state:%s, minimum_phi_value:%s\",\n",
    "        num_topics,\n",
    "        chunksize,\n",
    "        passes,\n",
    "        update_every,\n",
    "        alpha,\n",
    "        eta,\n",
    "        decay,\n",
    "        offset,\n",
    "        eval_every,\n",
    "        iterations,\n",
    "        gamma_threshold,\n",
    "        minimum_probability,\n",
    "        random_state,\n",
    "        minimum_phi_value,\n",
    "    )\n",
    "\n",
    "    intermediate_score_callback = IntermediateScoreCallback(\n",
    "        test_corpus=test_corpus,\n",
    "        test_df=test_filtered_df,\n",
    "        test_doc_df=test_doc_df,\n",
    "        dictionary=dictionary,\n",
    "        trial=trial,\n",
    "        logger=logger,\n",
    "        title=\"LDA\",\n",
    "    )\n",
    "\n",
    "    lda_model = LdaModel(\n",
    "        corpus=train_corpus,\n",
    "        id2word=dictionary,\n",
    "        num_topics=num_topics,\n",
    "        chunksize=chunksize,\n",
    "        passes=passes,\n",
    "        update_every=update_every,\n",
    "        alpha=alpha,\n",
    "        eta=eta,\n",
    "        decay=decay,\n",
    "        offset=offset,\n",
    "        eval_every=eval_every,\n",
    "        iterations=iterations,\n",
    "        gamma_threshold=gamma_threshold,\n",
    "        minimum_probability=minimum_probability,\n",
    "        random_state=random_state,\n",
    "        minimum_phi_value=minimum_phi_value,\n",
    "        callbacks=[intermediate_score_callback],\n",
    "    )\n",
    "\n",
    "    return intermediate_score_callback.score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "server_type = \"local\"\n",
    "# server_type = \"azure\"\n",
    "\n",
    "if server_type == \"local\":\n",
    "    # define storage using local postgresql database\n",
    "    storage = optuna.storages.RDBStorage(\n",
    "        url=\"postgresql://app_user:app_password@localhost:5432/app\",\n",
    "        heartbeat_interval=60,\n",
    "        grace_period=120,\n",
    "        failed_trial_callback=RetryFailedTrialCallback(max_retry=3),\n",
    "    )\n",
    "if server_type == \"azure\":\n",
    "    # define storage using azure postgresql database\n",
    "    storage = optuna.storages.RDBStorage(\n",
    "        url=\"postgresql://optuna:optunapswd@optimization-pg-server.postgres.database.azure.com:5432/optunadb\",\n",
    "        heartbeat_interval=60,\n",
    "        grace_period=120,\n",
    "        failed_trial_callback=RetryFailedTrialCallback(max_retry=3),\n",
    "    )\n",
    "\n",
    "# sampler = optuna.samplers.CmaEsSampler(\n",
    "#     seed=1, n_startup_trials=3, restart_strategy=\"ipop\"\n",
    "# )\n",
    "optimization_metric = \"MAP documents\"\n",
    "sampler = optuna.samplers.TPESampler(\n",
    "    seed=1, n_startup_trials=5, multivariate=True\n",
    ")\n",
    "\n",
    "STUDY_NAME = (\n",
    "    f\"optimize_lda_{TRAIN_DATASET_NAME}_on_{TEST_DATASET_NAME}\"\n",
    "    + f\"_{sampler.__class__.__name__}\"\n",
    "    # + f\"_{sampler._restart_strategy}\"\n",
    "    + f\"_v1\"\n",
    ")\n",
    "print(f\"study name:{STUDY_NAME}\")\n",
    "logger.info(\"study name:%s\", STUDY_NAME)\n",
    "\n",
    "# Set up the Optuna study oror load an existing one\n",
    "study = optuna.create_study(\n",
    "    study_name=STUDY_NAME,\n",
    "    sampler=sampler,\n",
    "    direction=\"maximize\",\n",
    "    storage=storage,\n",
    "    load_if_exists=True,\n",
    ")\n",
    "\n",
    "# Run the optimization\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=500,\n",
    "    catch=(ValueError),\n",
    "    gc_after_trial=True,\n",
    "    show_progress_bar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Retrieve the best hyperparameters and corresponding score\n",
    "best_params = study.best_params\n",
    "best_score = study.best_value\n",
    "\n",
    "# Print the best hyperparameters and score\n",
    "print(\"Best Hyperparameters: \", best_params)\n",
    "logger.info(\"Best Hyperparameters: %s\", best_params)\n",
    "print(\"Best Score: \", best_score)\n",
    "logger.info(\"Best Score: %s\", best_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
