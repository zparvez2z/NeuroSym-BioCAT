{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-25 13:57:36 47a7b3ad7f78 metapub.config[657] WARNING NCBI_API_KEY was not set.\n"
     ]
    }
   ],
   "source": [
    "# set environment variables\n",
    "import os\n",
    "# os.environ['NCBI_API_KEY'] = 'f017d0fb2594de7e4fb6c344d16c02d86a08'\n",
    "\n",
    "# import libraries\n",
    "import logging\n",
    "import json\n",
    "import pickle\n",
    "import glob\n",
    "import re\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from metapub import PubMedFetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directories(directories):\n",
    "    for directory in directories:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "            print(f\"Created directory: {directory}\")\n",
    "\n",
    "def get_unique_pmids(dataset_dir):\n",
    "    pmids = set()\n",
    "\n",
    "    for json_file in glob.glob(os.path.join(dataset_dir , \"*.json\")):\n",
    "        with open(json_file) as fp:\n",
    "            json_data = ''.join(fp)\n",
    "        data = json.loads(json_data)\n",
    "        data = data[\"questions\"]\n",
    "        # Extract all unique PMIDs from the documents list\n",
    "        for q in data:\n",
    "            for doc in q['documents']:\n",
    "                pmid = doc.split('/')[-1]\n",
    "                pmids.add(pmid)\n",
    "\n",
    "    print(len(pmids))\n",
    "    return pmids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-25 13:57:43 47a7b3ad7f78 __main__[657] INFO Logger initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset name:BioASQ-training9b\n",
      "train dataset dir:/workspace/data/raw/bioasq/2021/BioASQ-training9b\n",
      "train doc dir:/workspace/data/processed/bioasq/2021/BioASQ-training9b_documents/\n",
      "test dataset name:Task9BGoldenEnriched\n",
      "test dataset dir:/workspace/data/raw/bioasq/2021/Task9BGoldenEnriched\n",
      "test doc dir:/workspace/data/processed/bioasq/2021/Task9BGoldenEnriched_documents/\n",
      "log dir:/workspace/logs/notebooks\n",
      "model dir:/workspace/models/notebooks/bioasq/2021/BioASQ-training9b/\n",
      "results dir:/workspace/results/notebooks/bioasq/2021/BioASQ-training9b/\n",
      "LOG_FILE: /workspace/logs/notebooks/pubmed.log\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    # load dir_dict from json file in home directory\n",
    "    home_dir = os.path.expanduser(\"~\")\n",
    "    with open(f\"{home_dir}/.biomedqa_dir.json\", encoding=\"utf-8\") as fp:\n",
    "        dir_dict = json.load(fp)\n",
    "except Exception as exc:\n",
    "    print(\"Error: unable to load directory dictionary. Please run setup.py\")\n",
    "    raise exc\n",
    "\n",
    "# set directories\n",
    "BASE_DIR = dir_dict[\"base_dir\"]\n",
    "DATA_DIR = dir_dict[\"data_dir\"]\n",
    "MODEL_DIR = dir_dict[\"model_dir\"]\n",
    "LOG_DIR = dir_dict[\"log_dir\"]\n",
    "RESULTS_DIR = dir_dict[\"results_dir\"]\n",
    "\n",
    "DATASET = \"bioasq\"\n",
    "YEAR = \"2021\"\n",
    "__file__ = \"notebooks/pubmed.ipynb\"\n",
    "\n",
    "TRAIN_DATASET_NAME = \"BioASQ-training9b\"\n",
    "TRAIN_DATASET_DIR = f\"{DATA_DIR}/raw/{DATASET}/{YEAR}/{TRAIN_DATASET_NAME}\"\n",
    "TRAIN_DOC_DIR = (\n",
    "    f\"{DATA_DIR}/processed/{DATASET}/{YEAR}/{TRAIN_DATASET_NAME}_documents/\"\n",
    ")\n",
    "print(f\"train dataset name:{TRAIN_DATASET_NAME}\")\n",
    "print(f\"train dataset dir:{TRAIN_DATASET_DIR}\")\n",
    "print(f\"train doc dir:{TRAIN_DOC_DIR}\")\n",
    "\n",
    "TEST_DATASET_NAME = \"Task9BGoldenEnriched\"\n",
    "TEST_DATASET_DIR = f\"{DATA_DIR}/raw/{DATASET}/{YEAR}/{TEST_DATASET_NAME}\"\n",
    "TEST_DOC_DIR = (\n",
    "    f\"{DATA_DIR}/processed/{DATASET}/{YEAR}/{TEST_DATASET_NAME}_documents/\"\n",
    ")\n",
    "print(f\"test dataset name:{TEST_DATASET_NAME}\")\n",
    "print(f\"test dataset dir:{TEST_DATASET_DIR}\")\n",
    "print(f\"test doc dir:{TEST_DOC_DIR}\")\n",
    "\n",
    "# get file directory\n",
    "FILE_DIR = os.path.dirname(os.path.relpath(__file__))\n",
    "\n",
    "# set log dir directory according to current file directory\n",
    "LOG_DIR = f\"{LOG_DIR}/{FILE_DIR}\"\n",
    "print(f\"log dir:{LOG_DIR}\")\n",
    "\n",
    "# set model directory according to current file directory\n",
    "MODEL_DIR = f\"{MODEL_DIR}/{FILE_DIR}/{DATASET}/{YEAR}/{TRAIN_DATASET_NAME}/\"\n",
    "print(f\"model dir:{MODEL_DIR}\")\n",
    "\n",
    "# set results directory according to current file directory\n",
    "RESULTS_DIR = f\"{RESULTS_DIR}/{FILE_DIR}/{DATASET}/{YEAR}/{TRAIN_DATASET_NAME}/\"\n",
    "print(f\"results dir:{RESULTS_DIR}\")\n",
    "\n",
    "# create directories\n",
    "create_directories([LOG_DIR, TRAIN_DOC_DIR, TEST_DOC_DIR])\n",
    "\n",
    "# set log file name\n",
    "log_file = os.path.join(\n",
    "    LOG_DIR, os.path.basename(__file__).split(\".\")[0] + \".log\"\n",
    ")\n",
    "print(f\"LOG_FILE: {log_file}\")\n",
    "\n",
    "# initialize logger\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=log_file,\n",
    "    format=\"%(process)d\\t%(asctime)s\\t%(levelname)s\\t%(message)s\",\n",
    "    level=logging.DEBUG,\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"Logger initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33330\n",
      "3523\n"
     ]
    }
   ],
   "source": [
    "train_pmids = get_unique_pmids(TRAIN_DATASET_DIR)\n",
    "test_pmids = get_unique_pmids(TEST_DATASET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_articles(pmids, dataset_name, doc_dir):\n",
    "    \"\"\"\n",
    "    Fetch articles for given pmids and save to pickle file\n",
    "    \"\"\"\n",
    "    # check if pickle file already exists and get existing pmids\n",
    "    if os.path.exists(f\"{doc_dir}/{dataset_name}_documents_df.pkl\"):\n",
    "        df = pd.read_pickle(f\"{doc_dir}/{dataset_name}_documents_df.pkl\")\n",
    "        existing_pmids = set(df[\"pmid\"].values)\n",
    "        pmids = pmids - existing_pmids\n",
    "        print(f\"Number of pmids loaded from pickle file: {len(existing_pmids)}\")\n",
    "    else:\n",
    "        print(\"No pickle file found, fetching all articles\")\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "    # fetch articles one by one and add to dataframe and save to pickle file\n",
    "    fetch = PubMedFetcher()\n",
    "    for pmid in tqdm(pmids):\n",
    "        try:\n",
    "            article = fetch.article_by_pmid(pmid)\n",
    "            df = pd.concat(\n",
    "                [\n",
    "                    df,\n",
    "                    pd.DataFrame.from_dict(\n",
    "                        {\n",
    "                            \"abstractText\": article.abstract,\n",
    "                            \"journal\": article.journal,\n",
    "                            \"meshMajor\": [\n",
    "                                article.mesh[k][\"descriptor_name\"]\n",
    "                                for k in article.mesh.keys()\n",
    "                            ],\n",
    "                            \"pmid\": article.pmid,\n",
    "                            \"title\": article.title,\n",
    "                            \"year\": article.year,\n",
    "                        },\n",
    "                        orient=\"index\",\n",
    "                    ).T,\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "            # save dataframe to pickle file\n",
    "            df.to_pickle(f\"{doc_dir}/{dataset_name}_documents_df.pkl\")\n",
    "        except Exception as exc:\n",
    "            print(f\"Error fetching article: {pmid}\")\n",
    "            print(exc)\n",
    "        time.sleep(1 / 10)\n",
    "\n",
    "    # save dataframe to pickle file\n",
    "    df.to_pickle(f\"{doc_dir}/{dataset_name}_documents_df.pkl\")\n",
    "    # return dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc_df = fetch_articles(train_pmids, TRAIN_DATASET_NAME, TRAIN_DOC_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc_df = pd.read_pickle(f\"{TRAIN_DOC_DIR}/{TRAIN_DATASET_NAME}_documents_df.pkl\")\n",
    "train_doc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pmids loaded from pickle file: 3522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching article: 33097476\n",
      "Pubmed ID \"33097476\" not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_doc_df = fetch_articles(test_pmids, TEST_DATASET_NAME, TEST_DOC_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 6)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load test_doc_df from pickle file\n",
    "test_doc_df = pd.read_pickle(f\"{TEST_DOC_DIR}/{TEST_DATASET_NAME}_documents_df.pkl\")\n",
    "# show number of articles for which abstracts are not available\n",
    "test_doc_df[test_doc_df[\"abstractText\"].isnull()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "\n",
    "\n",
    "# funtion to scrap article from pubmed for which abstract is not available\n",
    "def scrap_null_article(DATASET_NAME, DOC_DIR):\n",
    "    # load doc_df from pickle file\n",
    "    df = pd.read_pickle(f\"{DOC_DIR}/{DATASET_NAME}_documents_df.pkl\")\n",
    "\n",
    "    # get pmids for which abstracts are not available\n",
    "    need_to_scrap = df[df[\"abstractText\"].isnull()][\"pmid\"].to_list()\n",
    "    print(\n",
    "        f\"Number of pmids for which abstracts are not available: {len(need_to_scrap)}\"\n",
    "    )\n",
    "\n",
    "    for pmid in tqdm(need_to_scrap):\n",
    "        try:\n",
    "            url = f\"https://pubmed.ncbi.nlm.nih.gov/{pmid}/\"\n",
    "            page = requests.get(url)\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "            journal = soup.select(\"#full-view-journal-trigger\")[\n",
    "                0\n",
    "            ].get_attribute_list(\"title\")[0]\n",
    "            mesh_major = [\n",
    "                item.get_text(strip=True).lower()\n",
    "                for item in soup.select(\"#mesh-terms > ul li div button\")\n",
    "            ]\n",
    "            year = soup.select(\n",
    "                \"#full-view-heading > div.article-citation > div.article-source > span.cit\"\n",
    "            )[0].text[:4]\n",
    "            abstract_text = \"\".join(\n",
    "                [\n",
    "                    paragraph.get_text(strip=True)\n",
    "                    for paragraph in soup.select(\"#eng-abstract > p\")\n",
    "                ]\n",
    "            )\n",
    "            pmid = re.findall(r\"/\\d+\", url)[0][1:]\n",
    "            title = soup.select(\"#full-view-heading > h1\")[0].get_text(\n",
    "                strip=True\n",
    "            )\n",
    "\n",
    "            df[df[\"pmid\"] == pmid] = [\n",
    "                abstract_text,\n",
    "                journal,\n",
    "                mesh_major,\n",
    "                pmid,\n",
    "                title,\n",
    "                year,\n",
    "            ]\n",
    "            # dump dataframe to pickle file\n",
    "            df.to_pickle(f\"{DOC_DIR}/{DATASET_NAME}_documents_df.pkl\")\n",
    "        except Exception as exc:\n",
    "            print(f\"Error fetching article: {need_to_scrap[0]}\")\n",
    "            print(exc)\n",
    "        time.sleep(random.randint(1, 3))\n",
    "\n",
    "    # dump dataframe to pickle file\n",
    "    df.to_pickle(f\"{DOC_DIR}/{DATASET_NAME}_documents_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pmids for which abstracts are not available: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "scrap_null_article(TEST_DATASET_NAME, TEST_DOC_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pmids for which abstracts are not available: 89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [04:26<00:00,  2.99s/it]\n"
     ]
    }
   ],
   "source": [
    "scrap_null_article(TRAIN_DATASET_NAME, TRAIN_DOC_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstractText</th>\n",
       "      <th>journal</th>\n",
       "      <th>meshMajor</th>\n",
       "      <th>pmid</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31229</th>\n",
       "      <td>BACKGROUND: Nicotine receptor partial agonists...</td>\n",
       "      <td>Cochrane Database Syst Rev</td>\n",
       "      <td>[Alkaloids, Azocines, Benzazepines, Bupropion,...</td>\n",
       "      <td>21154363</td>\n",
       "      <td>Nicotine receptor partial agonists for smoking...</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14948</th>\n",
       "      <td>BACKGROUND: Nicotine receptor partial agonists...</td>\n",
       "      <td>Cochrane Database Syst Rev</td>\n",
       "      <td>[Alkaloids, Azocines, Benzazepines, Bupropion,...</td>\n",
       "      <td>21328282</td>\n",
       "      <td>Nicotine receptor partial agonists for smoking...</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16164</th>\n",
       "      <td>Burosumab (Crysvita</td>\n",
       "      <td>Drugs</td>\n",
       "      <td>[Antibodies, Monoclonal, Antibodies, Monoclona...</td>\n",
       "      <td>29679282</td>\n",
       "      <td>Burosumab: First Global Approval.</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4074</th>\n",
       "      <td>Burosumab (Crysvita</td>\n",
       "      <td>Drugs Ther Perspect</td>\n",
       "      <td>[]</td>\n",
       "      <td>30459508</td>\n",
       "      <td>Burosumab in X-linked hypophosphatemia: a prof...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16132</th>\n",
       "      <td>CD4</td>\n",
       "      <td>Oncoimmunology</td>\n",
       "      <td>[]</td>\n",
       "      <td>24327937</td>\n",
       "      <td>Long peptide-based cancer immunotherapy target...</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32365</th>\n",
       "      <td>None</td>\n",
       "      <td>Expert Rev Clin Immunol</td>\n",
       "      <td>[Antirheumatic Agents, Arthritis, Rheumatoid, ...</td>\n",
       "      <td>30394138</td>\n",
       "      <td>Upadacitinib for the treatment of rheumatoid a...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32392</th>\n",
       "      <td>None</td>\n",
       "      <td>Oncotarget</td>\n",
       "      <td>[Animals, Antineoplastic Agents, B-Lymphocytes...</td>\n",
       "      <td>23455231</td>\n",
       "      <td>Attacking MALT1 for ABC-DLBCL therapy.</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32663</th>\n",
       "      <td>None</td>\n",
       "      <td>Infect Immun</td>\n",
       "      <td>[Animals, Antibodies, Bacterial, Bacterial Pro...</td>\n",
       "      <td>30201700</td>\n",
       "      <td>Histophilus somni Survives in Bovine Macrophag...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32686</th>\n",
       "      <td>None</td>\n",
       "      <td>Rev Bras Hematol Hemoter</td>\n",
       "      <td>[]</td>\n",
       "      <td>23741179</td>\n",
       "      <td>Neutropenic diet and quality of food: a critic...</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33075</th>\n",
       "      <td>None</td>\n",
       "      <td>Acta Obstet Gynecol Scand</td>\n",
       "      <td>[Adult, Cytomegalovirus Infections, Female, Fe...</td>\n",
       "      <td>17364293</td>\n",
       "      <td>Congenital cytomegalovirus infection presentin...</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>117 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstractText  \\\n",
       "31229  BACKGROUND: Nicotine receptor partial agonists...   \n",
       "14948  BACKGROUND: Nicotine receptor partial agonists...   \n",
       "16164                                Burosumab (Crysvita   \n",
       "4074                                 Burosumab (Crysvita   \n",
       "16132                                                CD4   \n",
       "...                                                  ...   \n",
       "32365                                               None   \n",
       "32392                                               None   \n",
       "32663                                               None   \n",
       "32686                                               None   \n",
       "33075                                               None   \n",
       "\n",
       "                          journal  \\\n",
       "31229  Cochrane Database Syst Rev   \n",
       "14948  Cochrane Database Syst Rev   \n",
       "16164                       Drugs   \n",
       "4074          Drugs Ther Perspect   \n",
       "16132              Oncoimmunology   \n",
       "...                           ...   \n",
       "32365     Expert Rev Clin Immunol   \n",
       "32392                  Oncotarget   \n",
       "32663                Infect Immun   \n",
       "32686    Rev Bras Hematol Hemoter   \n",
       "33075   Acta Obstet Gynecol Scand   \n",
       "\n",
       "                                               meshMajor      pmid  \\\n",
       "31229  [Alkaloids, Azocines, Benzazepines, Bupropion,...  21154363   \n",
       "14948  [Alkaloids, Azocines, Benzazepines, Bupropion,...  21328282   \n",
       "16164  [Antibodies, Monoclonal, Antibodies, Monoclona...  29679282   \n",
       "4074                                                  []  30459508   \n",
       "16132                                                 []  24327937   \n",
       "...                                                  ...       ...   \n",
       "32365  [Antirheumatic Agents, Arthritis, Rheumatoid, ...  30394138   \n",
       "32392  [Animals, Antineoplastic Agents, B-Lymphocytes...  23455231   \n",
       "32663  [Animals, Antibodies, Bacterial, Bacterial Pro...  30201700   \n",
       "32686                                                 []  23741179   \n",
       "33075  [Adult, Cytomegalovirus Infections, Female, Fe...  17364293   \n",
       "\n",
       "                                                   title  year  \n",
       "31229  Nicotine receptor partial agonists for smoking...  2010  \n",
       "14948  Nicotine receptor partial agonists for smoking...  2011  \n",
       "16164                  Burosumab: First Global Approval.  2018  \n",
       "4074   Burosumab in X-linked hypophosphatemia: a prof...  2018  \n",
       "16132  Long peptide-based cancer immunotherapy target...  2013  \n",
       "...                                                  ...   ...  \n",
       "32365  Upadacitinib for the treatment of rheumatoid a...  2019  \n",
       "32392             Attacking MALT1 for ABC-DLBCL therapy.  2012  \n",
       "32663  Histophilus somni Survives in Bovine Macrophag...  2018  \n",
       "32686  Neutropenic diet and quality of food: a critic...  2013  \n",
       "33075  Congenital cytomegalovirus infection presentin...  2007  \n",
       "\n",
       "[117 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find duplicate abstracts for train_doc_df\n",
    "train_doc_df[train_doc_df.duplicated(subset=['abstractText'], keep=False)].sort_values(by=['abstractText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find duplicate abstracts for test_doc_df\n",
    "test_doc_df[test_doc_df.duplicated(subset=['abstractText'], keep=False)].sort_values(by=['abstractText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate rows\n",
    "test_doc_df.drop_duplicates(subset=['pmid'], keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe to pickle file\n",
    "test_doc_df.to_pickle(f\"{TEST_DOC_DIR}/{TEST_DATASET_NAME}_documents_df.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
